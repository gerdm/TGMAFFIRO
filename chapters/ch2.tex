\documentclass[../TGMAFFIRO.tex]{subfiles}

\begin{document}
In the following chapter we present an overview of the theory of probability required to motivate and comprehend the rest of this work. We will present the intuition behind the definitions to use and the outcome of some proven theorems.

\section{Measurable Spaces}
Suppose we wanted to compute the probability of a fair coin landing two times heads. To do so, we may start by inquiring about all possible outcomes to arise. In this case,

\[
	\{HH\}, \ \{HT\}, \ \{TH\}, \ \{TT\}.
\]

The outcome of any possible event in a certain experiment is known as the \textbf{universe} of the experiment, and we denote it with $\Omega$. We can think about every element in this space ($\{\{HH\}, \ \{HT\}, \ \{TH\}, \ \{TT\}, \ \Omega\}$) as a possible question to ask. As we can see, the elements we currently have to answer questions about the outcomes of the experiment is constraint. For example, we are not taking account of the event in which tails do not land two times in a row: $\{TH, HT, HH\}$

% TODO Finish explanation on sigma algebra as a form to ask question of the given data

HH
HT
TH
TT

HH TH
HH HT
HT TH
HH TT
TH TT
HT TT

TH HT HH
TH HT TT
HH TH TT

omega
empty



\begin{definition}{\textbf{Algebra}}
	Let $\Omega$ be a set of points $\omega$. A system $\mathscr{A}$ of subsets of $\Omega$ is an Algebra if:
	\begin{enumerate}[I.]
		\item $\Omega \in \mathscr{A}$
		\item If $A, B \in \mathscr{A} \Rightarrow A \cap B \in \mathscr{A}$
		\item If $A \in \mathscr{A} \Rightarrow A^\mathsf{c} \in \salgF$
	\end{enumerate}
\end{definition}

\begin{definition}{\textbf{$\salg$}}\label{salgebra}
	A system $\salgF$ of subsets of $\Omega$ is a $\salg$ if it is an algebra and it satisfies one additional condition:
	\begin{equation*}
		\text{If } \{A_n\}_{n\geq 1} \in \salgF \Rightarrow \bigcup_{k=1}^{\infty} A_k \in \salgF 
	\end{equation*}
\end{definition}

\begin{remark}
	for $\{A_n\}_{n\geq 1} \in \salgF$ then, definition \ref{salgebra} is equivalent to say that
	\begin{equation*}
		\text{If } \{A_n\}_{n\geq 1} \in \salgF \Rightarrow \bigcap_{k=1}^{\infty} A_k \in \salgF
	\end{equation*}
\end{remark}

\begin{proof}
	By the generalized form of De Morgan's laws, $(\bigcap_k A_k)^\mathsf{c} \equiv \bigcup_k A_k^\mathsf{c}$. Since $A_k \in \salgF \ \forall \ k$ then, by definition,  $A_k^\mathsf{c} \in \salgF \ \forall \ k$ which implies  $\bigcup_k A_k^\mathsf{c}$ and this complement is also in $\salgF$
\end{proof}

\begin{definition}\label{salgebra_generated}
	Let $A \subseteq \Omega$ then, $\salgF_A = \{\emptyset, \Omega, A, \bar{A}\}$ is known as the $\salg$ generated by $A$. It is also denoted as $\sigma\{A\}$
\end{definition}

Defintion \ref{salgebra_generated} is, by definition, the smallest $\salg$ that cointains $A$. This definition is useful in the construction of one particular $\salg$ of subsets of $\RNums$.\\

Consider the collection of open intervals $(a,b) \in \RNums \ \forall \ a \leq b$. The smallest $\salg$ generated by this collection is known as the Borel $\salg$ of subsets of $\RNums$.

\begin{definition}{\textbf{The Borel $\salg$}}\label{borel_sigma_alg}
		\begin{equation}
			\borelsalg := \sigma\{(a,b) \subseteq \RNums:a\leq b\}
		\end{equation}
\end{definition}

Definition \ref{borel_sigma_alg} is the smallest $\salg$ that contains all open intervals in the real line. Furthermore, it can be proven that $[a, b]$, $(a,\infty)$, $(-\infty, b)$, $[a, b)$, $(a, b]$ are all elements in $\borelsalg$. 

\begin{definition}{\textbf{Measurable Space}}
	If $\salgF$ is a $\salg$ of subsets of $\Omega$, $(\Omega, \salgF)$ is said to be a measurable space. If $A$ is a set in $\salgF$, we say that $A$ is $\salgF$-measurable (or measurable w.r.t. $\salgF$)
\end{definition}

\begin{definition}{\textbf{Measure}}
	Let $\bar{\mathbb R} := \mathbb R \cup \{-\infty, \infty\}$ be the extended real line and let $(\Omega, \salgF)$ be a measurable space. It is said that a function $\mu: \salgF \rightarrow \bar{\mathbb R}$ is a measure over $(\Omega, \salgF)$ if
	\begin{enumerate}[I.]
		\item $\mu(\emptyset) = 0$
		\item $\mu(A) \geq 0\ \forall \ A \in \salgF $
		\item $\mu$ is $\sigma$-additive: If $\{A_k\}_{k=1}^{\infty}$ is a set of disjoint events in $\salgF$ ($A_i \cap A_j = \emptyset \ \forall \ i \neq j$), 
		\begin{equation*}
			\bigcup_{k=1}^{\infty} \mu(A_k) = \mu(\sum_{k=1}^{\infty} A_k)
		\end{equation*}
	\end{enumerate}
\end{definition}


If this last definition holds true then, $(\Omega, \salgF, \mu)$ is said to be a \textbf{measure space}. $\mu(A)$ is the measure of $A\in\salgF$. Furthermore, if $\mu(\Omega) < \infty$ then, $\mu$ is said to be a finite measure. Lastly, if  $\mu(\Omega) = 1$ then, $\mu$ is said to be a probability.\\

If $\mu$ is a probability then, $\mu \equiv \Pm$ is said to be a probability space. We can formalize this definition as follows:

\begin{definition}{\textbf{Kolmogorov's Axiom System}}
	An ordered triple $(\Omega, \salgF, \Pm)$ where
	\begin{itemize}
		\item $\Omega$ is a set of points $\omega$
		\item $\salgF$ is a $\salg$ of subsets of $\Omega$
		\item $\mathbb P$ is a probability on $\salgF$
	\end{itemize}
	is called a \textbf{probability space} or \textbf{model} (of an experiment)
\end{definition}

\section{Random Variables}
\begin{definition}{\textbf{Random Variable}}\label{rvar}
	Let $(\Omega, \salgF, \mu)$ be a measure space. The function $X: \Omega \rightarrow \RNums$ is an $\salgF$-measurable function if
	\begin{equation}
		X^{-1}(B) = \{\omega \in \Omega \ | \ X(w) \in B\} \in \salgF \ \ \forall \ \ B \subseteq \RNums
	\end{equation}
\end{definition}

\begin{definition}\label{dist_func}
	Consider the event $\{X \leq x\} := X^{-1}(-\infty, x]$. The function $F_X: \RNums \rightarrow [0, 1]$ defined as:
	\begin{equation}
		F_X(x) := \Pm\{X \leq x\}
	\end{equation}
	is known as the \textbf{distribution function} of the random variable $X$.
\end{definition}

Furthermore, a probability distribution function has the following properties:
\begin{itemize}
	\item For $x \leq y$ then, $F_X(x) \leq F_X(y)$
	\item $\lim\limits_{x\to\infty} F_X(x) = 1$ and $\lim\limits_{x\to - \infty} F_X(x) = 0$
	\item $F_X$ is right-continuous, meaning that $\lim\limits_{y\to x^+} F_X(y) = F_X(x)$ for every $x \in \RNums$
\end{itemize}

\begin{definition}
	A random variable $X$ is said to be \textbf{absolutely continuous} is there exists a nonnegative borel-measurable function $f: \RNums\rightarrow\RNums$ such that 
	\begin{equation}
		F_X(x) = \int_{-\infty}^{x} f(t) dt
	\end{equation}
\end{definition}

% TODO: Add example of normal distribution
The function $f$ is known as the \textbf{probability density function} of the random variable $X$.

\section{Lebesgue Integrals and Expectations}
\begin{definition}
	Let $(\Omega, \salgF, \mu)$ be a measure space and $X: \Omega \to \RNums$ an $\salgF-measurable function$. Denote $\ind_A(\omega)$ the indicator function of $\omega$ in a set $A$. That is,
	\[
	\ind_A(\omega) = 
		\begin{cases}
			1 & \omega \in A \\
			0 & \omega \notin A
		\end{cases}
	\]
	To simplify the notation, we will also denote $\mathbbm{1}_A(\omega)$ as $\ind_A$.
	We define the \textbf{Lebesgue integral} of $\ind_A$ with respect to (w.r.t.) $\mu$ as
	\begin{equation}
		\int_\Omega \ind_A(\omega) d\mu(\omega) := \mu(A),
	\end{equation}
	
\end{definition}

\begin{definition}
	Let $(\Omega, \salgF)$ a measurable space and $X$ an $\salgF$-measurable function. It is said that $X$ is a \textbf{simple function} if it takes only a unique finite number of values $\{x_i\}_{i=1}^{n} \in \RNums$. Then, $X$ can be written as
	\begin{equation}
		X(\omega) = \sum_{i=1}^{n}x_i\ind_{A_i}
	\end{equation}
	Where,
	\[
		A_i := \{\omega \in \Omega | X(\omega) = x_i\} \ \forall \ i = 1, \ldots, n
	\]
\end{definition}

\begin{definition}
	let $X$ be a simple function, we define the integral of $X$ w.r.t. $\mu$ as:
	\begin{equation}
		\int_\Omega X(\omega) d\mu(\omega) := \sum_{i=1}^{n}x_i\mu(A_i)
	\end{equation}
\end{definition}


%% Simple Functions aproximations %%
\begin{figure}[h]
	\centering
	\label{fig:simple_approx_sine}
	\includegraphics[width=0.9\textwidth]{images/simple_sine.pdf}	
	\caption{Approximation of a Borel-Measurable Function}
\end{figure}

%% Nonnegative lebesgue measure for nonnegative simple functions
\begin{proposition} \label{prop:nnlmsf}
	Let $X$ be a simple function defined on a measurable space $(\Omega, \salgF, \mu)$, in which $\mu(\Omega) < \infty$. Then,
	\begin{equation}
		X \geq 0 \implies \int_\Omega X d\mu \geq 0
	\end{equation}
\end{proposition}

\begin{proof}
	Since $X$ is a simple function,
	\[
	\int_\Omega X d\mu = \sum_{i=1}^n x_i \mu(A_1),
	\]
	where $x_i \geq 0 \ \forall \ i \in \{1, \ldots, n\}$. Also, $\mu(A_i) \geq 0 \ \forall \ A_i \in \salgF$.
\end{proof}

\begin{theorem}
	If $X$ and $Y$ are two non-negative simple functions and $\alpha, \beta \geq 0$,
	\begin{equation}
		\int_\Omega (\alpha X + \beta Y) d\mu = \alpha\int_\Omega X d\mu + \beta\int_\Omega Y d\mu;
	\end{equation}
	
	If $X \leq Y$ then,
	\begin{equation}
		\int_\Omega X \leq \int_\Omega Y
	\end{equation} 
\end{theorem}

\begin{proof}
	Let, $X = \sum_{i=i}^{n}x_i\ind_{A_i}$ and $Y = \sum_{j=1}^{m}y_i\ind{B_i}$. Note that $\bigcup_i A_i = \bigcup_j B_j = \Omega$. Also, $\ind_A = \sum_{i=1}^{n}\ind_{A_i}$ since $\{A_i\}$, by definition of the lebesgue integral, is a set of disjoin elements. Then, $X$ and $Y$ can be written as
	\begin{align*}
		X	&= \sum_{i=1}^{n}\ind_{A_i\cap\Omega} \\
			&= \sum_{i=1}^{n}\ind_{A_i\cap\left(\cup B_j\right)}\\
			&= \sum_{i=1}^{n} \sum_{j=1}^{m} x_i \ind_{A_i \bigcap B_j}.
	\end{align*}
	
	Similarly for $Y$,
	\begin{align*}
		Y	&= \sum_{j=1}^{m} \sum_{i=1}^{n} y_i \ind_{B_j \bigcap A_i} \\
			&= \sum_{i=1}^{n} \sum_{j=1}^{m} y_i \ind_{A_i \bigcap B_j}.
	\end{align*}
	
	Then,
	\begin{align*}
		\alpha X + \beta Y  &= \alpha \sum_{i=1}^{n} \sum_{j=1}^{m} x_i \ind_{A_i \bigcap B_j} + \beta \sum_{i=1}^{n} \sum_{j=1}^{m} y_i \ind_{A_i \bigcap B_j} \\
				&= \sum_{i=1}^{n} \sum_{j=1}^{m} (\alpha x_i + \beta y_i) \ind_{A_i \bigcap B_j}
	\end{align*}
	
	Now, 
	\begin{align*}
		\int_\Omega\alpha X+\beta Y &= \sum_{i=1}^{n} \sum_{j=1}^{m} (\alpha x_i + \beta y_i) \mu(A_i \cap B_j) \\
		&= \alpha \sum_{i=i}^{n}\left(\sum_{j=1}^{m} x_i \mu(A_i\cap B_j) \right) + \beta \sum_{j=1}^{m}\left(\sum_{i=1}^{n} x_i \mu(A_i\cap B_j) \right)\\
		&= \alpha \sum_{i=i}^{n}x_i \mu(A_i) + \beta\sum_{j=1}^{m} y_i \mu(B_i) \\
		&= \alpha \int_\Omega X d\mu + \beta\int_\Omega Y d\mu
	\end{align*}
	
	We now set out to prove the monotonicity property for nonnegative simple functions. 
	Let $X \leq Y$ then, $Y - X \geq 0$ is an $\salgF$-measurable function. By proposition \ref{prop:nnlmsf},
	\begin{equation}
		Y - X \geq 0 \implies \int_\Omega (Y - X) d\mu \geq 0
	\end{equation}	
\end{proof}

\begin{definition}
	For any nonnegative function $X$, we define the integral of $X$ w.r.t. a measure $\mu$ as
	\begin{equation}
		\int_\Omega Xd\mu := \sup\left\{\int_\Omega h d\mu \ | \ 0 \leq h \leq X \text{, $h$ is simple}\right\}
	\end{equation}
\end{definition}

If $X$ is any measurable function, we define the negative and positive parts of $X$:
\begin{align}
	X^+ := \max\{X(\omega), 0\} \\
	X^- := \max\{-X(\omega), 0\}
\end{align}

Thus, $X = X^+ - X^-$, and $|X| = X^+ + X^-$. Since both parts of $X$ are positive, their integrals are well defined.\\

If both $\int_\Omega X^+ d\mu$ and $\int_\Omega X^- d\mu$ are finite, we say that $X$ is integrable w.r.t. $\mu$. By linearity, 

\begin{equation}
	\int_\Omega X d\mu = \int_\Omega X^+ d\mu - \int_\Omega X^- d\mu.
\end{equation}

And,

\begin{equation}
	\int_\Omega |X| d\mu = \int_\Omega X^+ d\mu + \int_\Omega X^- d\mu.
\end{equation} \\

We will denote by $L_1 \equiv L_1(\Omega, \salgF, \mu)$ the family of integrable functions w.r.t. $\mu$. Note that $X \in L_1$ if and only if $|X| \in L_1$. i.e.,

\begin{align}
	\int_\Omega |X| d\mu = \int_\Omega X^+ d\mu + \int_\Omega X^- d\mu < \infty
\end{align}

If $A \in \salgF$, we define
\begin{equation}
	\int_A X d\mu := \int_\Omega X \ind_A d\mu
\end{equation}

%TODO: Left for the reader to do?

\begin{proposition} \label{prop:lebesgue_integral_props}
	For any $X$, $Y$ $\salgF$-measurable functions in $L_1$, $A$, $B$ members of $\salgF$, it can be proven that:
	\begin{equation}
		X \geq 0 \implies \int_\Omega X d\mu \geq 0 \\
	\end{equation}
	
	If $X \leq Y$ (monotonicity),
	\begin{equation}
		\int_\Omega X d\mu \leq \int_\Omega Y d\mu
	\end{equation}
	
	If $A \subseteq B$ and $X \geq 0$,
	\begin{equation}
		\int_A X d\mu \leq \int_B X d\mu
	\end{equation}
	
	If $\alpha$, $\beta$ $\in \RNums$ (linearity),
	\begin{equation}
	\int_\Omega (\alpha X + \beta Y) d\mu = \alpha \int_\Omega X d\mu + \beta \int_\Omega Y d\mu
	\end{equation}
\end{proposition}

\begin{theorem} \label{th:salg-lebesgue-int}
	Let $(\Omega, \salgF, \mu)$ be a measurable space, and $X$ a nonnegative $\salgF$-measurable function. If $X: \Omega \to \RNums$, the mapping from $\Omega \to [0, \infty]$ given by $A \to \int_A f d\mu$ is $\sigma$-additive, i.e.
	\begin{equation}
		\int_A X d\mu = \sum_{i=1}^{\infty}\int_{A_i} X d\mu
	\end{equation}
	For disjoint sets $\{A_i\}_{i\geq1}$
\end{theorem}

For a proof of theorem \ref{th:salg-lebesgue-int}, see \aycite{applebaum}. This last theorem is important in the sense that the treatment of lebesgue integrals can be taken as a measure. One consequence of this last one is the following corollary

\begin{corollary} \label{cor:partition_limit_integral}
	Let $X: \Omega \to \RNums$ a nonnegative measurable function and $\{E_n\}_{n\geq 1}$ a sequence of sets in $\Omega$ such that $E_n \subseteq E_{n+1}$ for every $n \in \mathbb{N}$. Denote $E = \bigcup_{n=1}^{\infty} E_n$. Then,
	\begin{equation}
		\int_E X d\mu = \lim_{n\to \infty} \int_{E_n} X d\mu
	\end{equation}
\end{corollary}

\begin{proof}
	Denote $A_1$ = $E_1$, and $A_n = E_n - E_{n-1} \ \forall \ n \geq 2$. The set $A_n$ is a sequence of disjoint sets. Furthermore, $E = \bigcup_{i=1}^\infty A_i$ and $E_n = \bigcup_{i=1}^n A_i \ \forall \ n \in \mathbb{N}$.\\
	
	By corollary \ref{cor:partition_limit_integral},
	
	\begin{align*}
		\int_E X d\mu &= \int_{\bigcup_{i=1}^{\infty} A_i} X d\mu \\
		&= \sum_{i=1}^{\infty} \int_{A_i} X d\mu \\
		&= \lim_{n\to\infty} \sum_{i=1}^{n} \int_{A_i} X d\mu \\ 
		&= \lim_{n\to\infty} \int_{\bigcup_{i=1}^{n} A_i} X d\mu \\
		&= \lim_{n\to\infty} \int_{E_n} X d\mu
	\end{align*}
\end{proof}


\begin{theorem} (\textbf{Monotone Convergence Theorem})
Let $\{X_n\}_{n\geq 1}$ be a sequence of nonnegative functions where $X_i: \Omega \to \RNums$ such that $X_n \leq X_{n+1}$ $\forall \ n \in \mathbb{N}$. Let $X$ be another random variable such that $X_n \xrightarrow[n \to \infty]{}X$ then,
\begin{equation}
	\int_\Omega X d\mu = \lim_{n\to \infty} \int_\Omega X_n d\mu
\end{equation}
\end{theorem}

\begin{proof}
	Since $X = \sup_{n\in \mathbb{N}}X_n$, by monotonicity,
	\begin{equation*}
		\int_\Omega X_1 d\mu \leq \int_\Omega X_1 d\mu \leq \ldots \leq \int_\Omega X d\mu
	\end{equation*}
	Thus,
	\begin{equation*}
		\lim_{n\to\infty}\int_\Omega X_n d\mu \leq \int_\Omega X d\mu
	\end{equation*}
	
	To show the reverse inequality, let $0 \leq h \leq X$ a a simple function and let $c \in (0,1)$. Denote $A_n = \{\omega\in\Omega \ | \ X_n(\omega) \geq c\cdot h(\omega)\}$.\\
	
	Since $\{X_n\}_{n\geq 1}$ is a sequence of nondecreasing functions, $A_n \subseteq A_{n+1}$ $\forall$  $n\in\mathbb{N}$. One final property of $\{A_n\}$ is the fact that $\bigcup_nA_n = \Omega$.	This last property can be shown by noticing that each $A_n$ has two possible outcomes to determine whether any $\omega\in\Omega$ is inside $A_n$:\\
	
	Consider $h(\omega)=0$. In that case, any $A_k = \{\omega\in\Omega \ | \  X_k(\omega) \geq 0\}$ will capture every element $\omega$. Consider now $h(\omega) \neq 0$. In this case, we do not know for which $k\in\mathbb{N}$ will $c\cdot h \leq X_k$, nonetheless, since $X_n \to X$, for some $k\in\mathbb N$, $X_k(\omega) \leq c\cdot h(\omega)$.
	
	By proposition \ref{prop:lebesgue_integral_props},
	\begin{equation}
		\lim_{n\to\infty}\int_\Omega X_n d\mu \geq \int_\Omega X_n d\mu \geq \int_{A_n} X_n d\mu \geq \int_{A_n} c\cdot h d\mu
	\end{equation}
	
	As $A_n$ is an increasing sequence,
	\begin{equation}
		\lim_{n\to\infty}\int_\Omega X_n d\mu \geq \lim_{n\to\infty}\int_{A_n} c\cdot h d\mu
	\end{equation}
	
	Taking account of the linearity property for lebesgue integrals and corollary \ref{cor:partition_limit_integral},
	\begin{equation}
  		\lim_{n\to\infty}\int_{A_n} c\cdot h d\mu = c\int_{\Omega} \cdot h d\mu
	\end{equation}
	
	Choosing an arbitrary $c \in (0,1)$, set $c= 1 - \frac{1}{k}$ and let $k\to\infty$,
	\begin{equation}
  		\lim_{n\to\infty}\int_\Omega X_n d\mu \geq \lim_{k\to\infty} \left(1 - \frac{1}{k}\right)\int_{\Omega} h d\mu = \int_{\Omega} \cdot h d\mu
	\end{equation}
	
	Choosing and arbitrary $h$, we find that
	\begin{equation}
  		\lim_{n\to\infty}\int_\Omega X_n d\mu \geq \sup\left\{\int_{\Omega} h d\mu\right\} = \int_\Omega X d\mu
	\end{equation}
\end{proof}

\begin{theorem} \textbf{Bounded Convergence Theorem}
\end{theorem}

\begin{theorem} \textbf{Fatou's Lemma}
\end{theorem}

\begin{theorem} \textbf{Dominated Convergence Theorem}
\end{theorem}


\begin{definition} (\textbf{Expectation})
	Let $X$ be a random variable on $(\Omega, \salgF, \Pm)$. The integral of $X$ w.r.t. the measure $\Pm$ is known as the expectation of $X$ w.r.t. $\Pm$. 
	\begin{equation}
		\mathbb{E}_{\Pm}[X] := \int_\Omega X d\Pm
	\end{equation}
\end{definition}


\begin{theorem}\label{h1_borel}
		Let $X$ be a random variable on $(\Omega, \salgF, \Pm)$ and $h: \RNums \to \RNums$ a Borel-measurable function. Let $F_X$ be the distribution of $X$ and $P_X(B):= \Pm[X^{-1}(B)]$ for every $B \in \borelsalg$ the probability measure induced by $X$ on $(\RNums, \borelsalg)$ then,
		\begin{equation}
			h \in L_1(\Omega, \salgF, \mathbb{P}) \iff h \in L_1(\RNums, \borelsalg, P_X)
		\end{equation}
\end{theorem}

To prove theorem \ref{h1_borel} we will make use of the so called ``\textbf{standard machine}'' approach. This approach will let us prove the theorem for simple functions then, by linearity, it is true for simple function and finally, we conclude by the monotone convergence theorem.\\

\begin{proof}
Let $X$ be a random variable on $(\Omega, \salgF, \Pm)$ and let $h$ be the indicator function for some set $B \in \borelsalg$. Then, for every $\omega \in \Omega$, $\ind_B\left(X(\omega) \right) = 1$ iff $\omega \in X^{-1}(B) := \{\omega \in \Omega | X(\omega) \in B\}$. This is because for $\ind_B(X(\omega))$ to be 1, the value $X(\omega)$ in $\borelsalg$, say $x$, is an element in $B$ if and only if, the inverse image of $X$ over $B$ i.e. the set of elements $\omega$ that comprise $B$ through a mapping from $X$, has $\omega$ as an element in it.\\

From this on,
\begin{equation*}
	\int_\Omega \ind_B\left(X(\omega)\right) d\Pm(\omega) = \int_\Omega \ind_{X^{-1}(B)} (\omega) d\Pm(\omega) = \int_{X{^-1}(B)} d\Pm(\omega)
\end{equation*}

Then, by definition,
\begin{equation}
	\int_{X^{-1}(B)} d\Pm(\omega) = \Pm\left[X^{-1}(B)\right] = P_X(B)
\end{equation}

And it follows,
\begin{equation*}
	P_X(B) = \int_\RNums \ind_{B}(x) dP_X = \int_\RNums h(x) dP_X(x)
\end{equation*}

Assume $h$ is now a simple function, $h(\cdot) =: h_n(\cdot) = \sum_{i=1}^n \alpha_i\ind_{B_i}(\cdot)$. Then,

\begin{align*}
	\int_\Omega h_n (X(\omega)) d\Pm &= \int_\Omega \sum_{i=1}^{n}\alpha_i \ind_{B_i}(X(\omega)) d\Pm\\
	\intertext{by linearity,}
	&= \sum_{i=1}^{n}\alpha_i\int_\Omega\ind_{B_i}(X(\omega)) d\Pm\\
	&= \sum_{i=1}^n\alpha_i\int_\RNums\ind_{B_i}(x) P_X\\
	&= \int_\RNums\sum_{i=1}^n\alpha_i\ind_{B_i}(x) dP_X\\
	&= \int_\RNums h_n(x) dP_x
\end{align*}

Finally, let $h$ be a nonnegative Borel-Measurable function. Then, there exists a nondecreasing sequence of simple functions such that $h_n \to h$ as $n \to \infty$,

\begin{align*}
	\int_\Omega h((X(\omega)) d\Pm &= \int_\Omega \lim_{n\to\infty} h_n(X(\omega)) d\Pm \\
	\intertext{By the monotone convergence theorem,}
	&= \lim_{n\to\infty} \int_\Omega h_n(X(\omega)) d\Pm\\
	&= \lim_{n\to\infty} \int_\RNums h_n(x) dP_x\\
	&= \int_\RNums \lim_{n\to\infty} h_n(x) dP_x\\
	&= \int_\RNums h(x) dP_x
\end{align*}
\end{proof}

Theorem \ref{h1_borel} connects the abstract space $\Omega$ to a more tractable space $\RNums$.\\

Now, recall that if $X$ is any random variable with density function $f(x)$ then, for any $B \in \borelsalg$,
\begin{equation*}
	P_X(B) = \int_\RNums f(x) dx
\end{equation*}

The same argument as theorem \ref{h1_borel} results in the following corollary,
\begin{corollary}
	let $X$ be a random variable with density $f(x)$ and let $h$ be a borel-measurable function $h$ such that. $\mathbb{E}[h(X)] < \infty$ then,
	\begin{equation}
		\mathbb{E}[h(X)] = \int_\RNums f(x) dx
	\end{equation}
\end{corollary}

\begin{theorem}\textbf{Radon-Nikodym Theorem}
Let $\mu$ and $\lambda$ be $\sigma$-finite positive measures defined on $(\Omega, \mathscr{F})$ such that for every $A \in \mathscr{F}$, $\lambda(A) = 0 \implies \mu(A)= 0$. Then, there exists a function $f: \Omega \to [0, \infty]$ such that
\[
	\mu(A) = \int_A f d\lambda
\]

Where the function $f$ is defined up to sets with measure zero. $f$ is sometimes called the Radon-Nikodym derivative and it can be written as $\frac{d\mu}{d\lambda}$
\end{theorem}

\section{Conditional Expectations}

\begin{definition}\textbf{Conditional Expectation w.r.t. a $\sigma$-algebra}
The conditional expectation of a nonnegative random variable $X$ with respect to the $\sigma$-algebra $\mathscr{G}$ is a nonnegative random variable denoted $\mathbb{E}[X | \mathscr{G}]$ or $\mathbb{E}[X | \mathscr{G}](\omega)$ such that,
\begin{enumerate}[I.]
	\item $\mathbb{E}[X | \mathscr{G}]$ is $\mathscr{G}$-measurable;
	\item For every $A \in \mathscr{G}$,
	\[\int_A X d\Pm = \int_A \mathbb{E}[X | \mathscr{G}] d\Pm \]
\end{enumerate}

\end{definition}
Note that the existence of this random variable is given by the Radon-Nikodym theorem.

\begin{definition}\textbf{Conditional Probability}.
	Let $B\in\salgF$. The conditional expectation $\mathbb{E}[B | \mathscr{G}] := \Pm(B | \mathscr{G})$ is defined as the conditional probability of $B$ w.r.t. the $\salg$ $\mathscr{G} \subseteq \salgF$
\end{definition}

\begin{definition}
	Let $X$ be a random variable and consider the $\salg$ generated by some random variable $Y$. Then,
	\begin{equation}
		\E[X | \sigma\{Y\}] =: \E[X | Y]
	\end{equation}
	For $B \in \salgF$,
	\begin{equation}
		\Pm[X | \sigma\{Y\}] =: \Pm[X | Y]
	\end{equation}
\end{definition}

\begin{proposition}
	We now note some important properties of the conditional expectation
	\begin{enumerate}
		\item If $a, b \in \RNums$, $\E[\alpha X + \beta Y | \mathscr{G}]$ = $\alpha \E[X | \mathscr{G}] + \beta\E[Y | \mathscr{G}]$
		\item $\E[\E[X | \salgF]] = \E[X]$
		\item $\E[\E[X | \salgF_2] | \salgF_1]$ = $[X | \salgF_1]$ if $\salgF_1 \subseteq \salgF_2$
		\item $\E[\E[X | \salgF_2] | \salgF_1]$ = $[X | \salgF_2]$ if $\salgF_2 \subseteq \salgF_1$
		\item Let $X$ be a random variable such that $X \perp \mathscr{G}$ then, $\E[X|\mathscr{G}] = \E[X]$. Consequently, for any borel-measurable function $h$, $\E[h(X)|\mathscr{G}] = \E[h(X)]$
	\end{enumerate}
\end{proposition}

%TODO: Add proooooooooooooooooofs!

\section{Stochastic Processes, Filtrations \& Martingales}

\begin{definition}\textbf{Stochastic Process}
	Let $(\Omega, \salgF, \Pm)$ be a probability space. A stochastic process is a set of random variables that takes values in some set $S$ called the state space, and are indexed by some set $T$.
	\begin{equation}
		\{X_t \ | \ t \in T\}
	\end{equation}
\end{definition}

For $S$ to make sense, it must be measurable with respect to some $\salg$. In particular, we will consider $S \subseteq \RNums$ where
\[
	S \cap \borelsalg \neq \emptyset.
\]

Note that $T$ can be any arbitrary set, nonetheless, we will work with only two state spaces:
\begin{enumerate}
	\item if $T = \{0, 1, \ldots, \}$, we say that the process is a \textbf{discrete time process}. For notation purposes, we will denote a discrete time process as
	\begin{equation}
		\{X_n\}_{n\geq 0}:= \{X_n \ | \ n \in \{0, 1, \ldots, \}\}
	\end{equation}
	
	\item if $T = \{t \ | \ t\in\RNums^+\}$, the process is said to be a \textbf{continuous time process}. In this case, we will denote this process as the one indexed by the letter $t$,
	\begin{equation}
		\{X_t\}_{t\geq 0} := \{X_t \ | \ t \in \RNums^+\}
	\end{equation}
\end{enumerate}

With this in mind, we can consider a stochastic process as a two variable function
\[
	X: T\times\Omega \to S
\]

Where,
\begin{enumerate}
	\item For every $t\in T$, $\omega \to X_t(\omega)$ is random variable;
	\item for every $\omega \in \Omega$, $t \to X_t(\omega)$ is the path of the process.
\end{enumerate}

% TODO: insert example of how would it look to see a path v.s. how would it 
% look to see a random variable. i.e. distribution v.s. path on a graph

\begin{definition}
Let $(\Omega, \salgF, \Pm)$ a probability space. A filtration is a non-decreasing sequence of sub-$\salg$s of $\salgF$, $\{\salgF_n\}_{n\geq 0}$ such that 
\[
	\salgF_t \leq \salgF_s \ \forall \ 0 \leq t \leq s
\]
\end{definition}

The stochastic process $\ctspr$ is said to be \textbf{adapted} to the filtration $\{\salgF_t\}_{t\geq 0}$ if $X_t$ is $\salgF_t$-measurable for every $t\geq 0$\\

The minimal $\salg$ generated by $\ctspr$ at time $\tau$ is known as the \textbf{natural filtration} of $\ctspr$ at time $\tau$. Note that $\ctspr$ is always adapted to its natural filtration at time $t$. That is $X_t$ is $\sigma(\{X_s | s \leq t\})$-measurable.\\

In a discrete time process, we denote the minimal $\salg$ generated by $\dtspr$ up to time $k$ as $\{X_1, \ldots, X_k\} := \sigma(\{X_1, \ldots, X_k\})$

	
\end{document}
