\documentclass[../TGMAFFIRO.tex]{subfiles}

\begin{document}
\section{It\^o's Integral}
Let $\ProbSpace$ a probability space and consider the spaces $L_2\ProbSpace$ and $L_2([a, b]\times\Omega, \mathscr{B}\otimes\salgF)$ with inner product
\begin{equation}
  \innerprod{f}{g} := \int_\Omega\int_0^t f(s) g(s) ds d\Pm = \Exp{\int_0^t f(s)g(s) ds},
\end{equation}


and norm in $L_2([a, b] \times \Omega)$
\begin{equation}
  \norm{f}_{[a, b] \times \Omega} := \sqrt{\innerprod{f}{f}} = \sqrt{\Exp{\int_0^t f(s)g(s) ds}}.
\end{equation}

We want to make sense of
\begin{equation}
  \int_a^bf(t,\omega) \dW[t](\omega).
\end{equation}

Where $W_t$ is a standard Brownian motion and $f$ belongs to a special family of functions denoted by $\Nfam{a}{b}$.

\begin{definition}
	Let $0\leq a\leq b$. Denote $\Nfam{a}{b}$ the family of stochastic process such that
	\begin{enumerate}
		\item $(t, \omega) \to f(t, \omega):[0, \infty]\times\Omega \to \RNums$ is $\borelsalg\times\salgF$ measurable;
		\item $f(t,\omega)$ is $\salgF_t$-adapted; and
		\item $f\in L_2([a,b]\times\Omega)$ i.e. $\int_a^b\int_\Omega f^2 d\Pm dt = \int_a^b\Exp{f^2}dt = \Exp{\int_a^b f^2 dt} < \infty$.
	\end{enumerate}
\end{definition}

\begin{definition}[\textbf{Simple Process}]
	We define the process $f\in \Nfam{a}{b}$ as simple if there exists a partition $\{t_i\}_{i=0}^n$ where $t_0 = a$, $t_n = b$, and $t_i < t_j \ \forall \ i < j$, such that $f$ can be represented as a lineal combination of the form
	\begin{equation}
		f(t) = \sum_{i=0}^{n-1}f(t_i)\ind_{[t_i, t_{i+1}]}(t).
	\end{equation}
\end{definition}

\begin{definition}
	We define the family of processes $f$ in $\Mfam{a}{b}$ that are processes in $\Nfam{a}{b}$ and can be written as simple processes. 
\end{definition}

\begin{definition}
	Let $f\in\Mfam{a}{b}$, we define the It\^o integral of $f$ as
\begin{equation}
	  \int_a^b f(t) dW_t := \sum_{i=0}^{n-1} \DeltaW .
\end{equation}

With $\Delta W_i := W_{t_{i+1}} - W_{t_{i}}$.\\
We will also denote It\^o's integral as $I(f)$ or $\int f$.
\end{definition}

\begin{figure}[hbt]
  \includegraphics[width=0.9\textwidth]{../images/simple_process}
  \caption{Simulation of Simple Process}
\end{figure}


\begin{proposition}
	Let $f$, $g$ $\in \Mfam{0}{t}$ then
	\begin{enumerate}
		\item $I(\alpha f + \beta g) = \alpha I(f) + \beta I(g)$;
		\item $\Exp{I(f)} = 0$; and
		\item $\Var{I(f)} = \Exp{\left(\int_0^t f(s) dW_s\right)^2} = \Exp{\int_0^t f^2(t) dt}$ (It\^o Isometry).
	\end{enumerate}
\end{proposition}

\begin{proof}
	1). Let $f,g\in\Mfam{0}{t}$, and $\alpha, \beta \in\RNums$. Then, we can write $f$ and $g$ as a simple process. Namely,
	\begin{align}
		f(t) = \sum_{i=0}^{n-1}f(t_i)\ind_{i}(t) \text{; and}\\
		g(t) = \sum_{j=0}^{m-1}g(t_j)\ind_{j}(t).
	\end{align}
	\newcommand{\fsimple}{\sum_{i=0}^{n-1}f(t_i)\ind_{i}(t)}
	\newcommand{\gsimple}{\sum_{j=0}^{m-1}g(t_j)\ind_{j}(t)}
	
	Where $\ind_i(t) := \ind_{t_{i}, t_{i+1}}(t)$. We now consider the following
	\begin{align*}
		\int_0^t(\alpha f(s) + \beta g(s))dW_s &= \int_0^t\left(\alpha \fsimple + \beta \gsimple \right) dW_s\\
		&= \sum_{k=0}^{n-1}\left(\alpha \fsimple + \beta \gsimple \right) \DeltaW[j]\\
		&= \alpha\sum_{k=0}^{n-1}\left(\fsimple\right)\Delta W_j + \\
		&\phantom{{}=1}\beta\sum_{k=0}^{n-1}\left(\gsimple\right)\DeltaW[j]\\
		&= \alpha\int_0^t f(s) dW_s + \beta \int_0^t g(s) \dW[s].
	\end{align*}
	
	2) For this next proof, we first note that $\Delta W_i$ is independent of its $\salg$. We can then write
	\begin{align}
		\E[\DeltaW| \salgF_{t_i}] &= \E[\DeltaW] = 0 \text{, and}\\
		\E[\DeltaW^2 | \salgF_{t_i}] &= \E[\DeltaW^2] = \E[\left(W_{t_{i+1}} - W_{t_{i}}\right)^2] = t_{i+1} - t_i.
	\end{align}
	Consider now
	\begin{align*}
	\Exp{f(t_i)\DeltaW} &= \Exp{\Exp{f(t_i)\DeltaW|\salgF_{t_i}}}\\
						&= \Exp{f(t_i)\Exp{\DeltaW|\salgF_{t_i}}}\\
						&= 0.
	\end{align*}
	It follows,
	\begin{align*}
		\Exp{I(f)} &= \Exp{\sum_{i=0}^{n-1}f(t_i)\DeltaW}\\
				   &= \sum_{i=0}^{n-1}\Exp{f(t_i)\DeltaW}\\
				   &= 0.
	\end{align*}
	
	We now consider
	\begin{align}
		\Exp{I(f)^2} &= \Exp{\left(\sum_{i=0}^{n-1} f(t_i) \DeltaW \right)^2}\label{eq:itois1} \\
					 &= \Exp{\sum_{i=0}^{n-1}f^2(t_i)\left(\DeltaW\right)^2 +  2\sum_{i<j}f(t_i)f(t_j) \DeltaW\DeltaW[j]}\nonumber \\
					 &= \sum_{i=0}^{n-1}\Exp{f^2(t_i)\left(\DeltaW\right)^2} + \label{eq:itois2}\\ 
					 &\phantom{{}=1} 2\sum_{i<j}\Exp{f(t_i)f(t_j) \DeltaW\DeltaW[j]}.\label{eq:itois3}
	\end{align}
	Where, (\ref{eq:itois3}):
	\begin{align*}
		\Exp{f(t_i)f(t_j) \DeltaW\DeltaW[j]} &= \Exp{\Exp{f(t_i)f(t_j) \DeltaW\DeltaW[j]|\salgF_{t_j}}}\\
			&= \Exp{f(t_i)f(t_j)\DeltaW\Exp{\DeltaW[j]|\salgF_{t_j}}}\\
			&= 0.
	\end{align*}
	
	We can rewrite (\ref{eq:itois1}) using only (\ref{eq:itois2}), i.e.,
	\begin{align*}
		\Exp{I(f)^2} &= \sum_{i=0}^{n-1}\Exp{f^2(t_i)\left(\DeltaW\right)^2}\\
		&= \sum_{i=0}^{n-1}\Exp{\Exp{f^2(t_i)\left(\DeltaW\right)^2|\salgF_{t_i}}} \\
		&= \sum_{i=0}^{n-1}\Exp{f^2(t_i)\Exp{\left(\DeltaW\right)^2|\salgF_{t_i}}} \\
		&= \sum_{i=0}^{n-1}\Exp{f^2(t_i)}\left(t_{i+1} - t_i\right) \\
		&= \int_0^t \Exp{f^2(s)} ds.
	\end{align*}
\end{proof}

\begin{remark}
	It\^o's integral as defined for $f\in\Mfam{a}{b}$ is a linear function, and indeed, an isometry since
	\begin{equation}
		\norm{I(f)}_{[a,b]} = \norm{f}_{[a, b]\times\Omega}.
	\end{equation}
	We can rewrite this last statement as
	\begin{equation}
		\Exp{\left(\int_0^t f dWs\right)} = \int_0^t\Exp{f^2(s)} ds = \Exp{\int_0^t f^2(s) ds}.
	\end{equation}	 
	
	Consequently, for any $f, g \in \Mfam{0}{t}$
	\begin{equation}
		\innerprod{I(f)}{I(g)}_{\Omega} = \innerprod{f}{g}_{\Omega\times[0, t]}.
	\end{equation}

To see why, we first note that $\innerprod{f}{g} = \int_0^t\Exp{f\cdot g}dt$. It follows that
\begin{align*}
	\innerprod{I(f)}{I(g)} &= \frac{1}{4}\left(\norm{I(f) + I(g)}^2 - \norm{I(f) - I(g)}^2\right)\\
						   &= \frac{1}{4}\left(\norm{I(f + g)}^2 - \norm{I(f - g)}^2\right)\\
						   &= \frac{1}{4}\left(\int_0^t \Exp{(f+g)^2}dt - \int_0^t \Exp{(f-g)^2}dt\right)\\
						   &= \frac{1}{4}\left(\int_0^t\Exp{f^2 + 2f\cdot g + g^2 - f^2 + 2f\cdot g - g^2} dt\right)\\
						   &= \int_0^t\Exp{f\cdot g} dt \\
						   &= \Exp{\int_0^t \left(f\cdot g\right) dt}\\
						   &= \innerprod{f}{g}.
\end{align*}
\end{remark}

%TODO: Add Proof
\begin{proposition}\label{prop:bounded_simple_borel}
	Let $f:[0, t]\to \RNums$ a Borel-measurable function such that $|f|\leq M$, and define $f_n:[0, t]\to \RNums$ as
	\begin{equation}
		f_n(\tau) := n\int_0^\tau e^{-n(\tau-s)}f(s) ds; \ 0 \leq \tau \leq t.
	\end{equation}
Then, $\{f_n\}$ is uniformly bounded by $M$ and $f_n \to f$ ($\mu$ a.e.) for every $\tau\in [0, t]$.
\end{proposition}

\begin{proposition}\label{prop:l2_convergence_ito_integral}
	The space $\Mfam{0}{t}$ is dense in $\Nfam{0}{t}$ under $L_2([a,b]\times\Omega)$. That is, for every $f\in\Nfam{0}{t}$, there exists a sequence $\{f_n\}_{n\geq 1} \in \Mfam{0}{t}$ such that
	\begin{equation}
		\Exp{\left(\int_0^t |f_n(s) - f(s)|dW_s\right)^2} \to 0.
	\end{equation}
	
\begin{proof}
We first note that 
\begin{equation}
	\Exp{\left(\int_0^t |f_n(s) - f(s)|dW_s\right)^2} = \Exp{\int_0^t\left|f_n(s) - f(s) \right|^2 ds} \to 0.
\end{equation}

\textbf{Step 1}: Continuous and bounded $f\in\Nfam{0}{t}$.\\
Let $g(\cdot, \omega)\in \Nfam{0}{t}$ continuous for each $\omega$. Then there exist a set $\{h_n\}_{n\geq 1}$ of simple processes in $\Mfam{0}{t}$ such that 
	\begin{equation}
		\Exp{\int_0^t\left(g - h_n\right)^2 ds}	\to 0.
	\end{equation}

Let $h_n(t, \omega) = \sum_{j=0}^{n-1}g(t_i)	\ind_{[t_i, t_{i+1}](t)}$. Then, $h_n$ is simple and,
\begin{equation}
  \lim_{n\to\infty} g - h_n = 0.
\end{equation}

Since $g - h_n$ converges to 0 (a.e.), the convergence is also in measure. Consequently, from the bounded convergence theorem we have that
\begin{equation}
  \lim_{n\to\infty} \int_0^t\left(g - h_n\right)^2 ds = 0 \ \forall \ \omega \in \Omega.
\end{equation}

Finally, by the dominated convergence theorem we see that
\begin{align*}
  \lim_{n\to\infty} \int_\Omega\left(\int_0^t\left(g - h_n\right)^2 ds\right)d\Pm &=  \int_\Omega \lim_{n\to\infty} \left(\int_0^t\left(g - h_n\right)^2 ds\right)d\Pm \\
  &= \Exp{\lim_{n\to\infty}\left(\int_0^t\left(g - h_n\right)^2 ds\right)} \\
  &= 0.
\end{align*}

\textbf{Step 2}: Bounded $f\in\Nfam{0}{t}$.\\
Let $f\in\Nfam{0}{t}$ be bounded i.e., there exists $M<\infty$ such that $|f(t, \omega) \leq M|$ for every $(t,\omega) \in [a,b]\times\Omega$. Then, there exists a sequence $\{f_n\}_{n\geq 1}$ of simple functions such that $f_n \to f$ in $L_2$.\\

By \ref{prop:bounded_simple_borel}, there exists $f_n$ such that $t\to f_n(t,\omega)$ is continuous for every $\omega\in\Omega$, $|f(t,\omega)|\leq M$, and $f_n(t,\Omega) \to f(t,\Omega)$. Since $f_n\to f$ a.e., then $f_n - f \to 0$ in measure, where we conclude by the bounded convergence theorem that
	\begin{equation}
		\Exp{\int_0^t(f_n - f)^2 ds} \to 0.
	\end{equation}

\textbf{Step 3}: Any $f\in\Nfam{0}{t}$.\\
For every $n\in\mathbb{N}$, define
	\begin{equation}
		f_n(t) = \begin{cases}
					f(t) & |f(t)|\leq n\\
					n 	 & f(t) > n \\
					-n   & f(t) < n
			     \end{cases}
	\end{equation}
It can be seen that $f_n\to f$. This implies a.e. convergence, and as a consequence, convergence in measure. Furthermore, $f_n$ is bounded by $f$ ($|f_n| \leq f$). The $L_2$ convergence follows from the dominated convergence.
\end{proof}
\end{proposition}

\begin{definition}[\textbf{It\^o's Integral for $f\in\Nfam{0}{t}$}]
	Let $f\in\Nfam{0}{t}$. By proposition \ref{prop:l2_convergence_ito_integral} there exists a sequence of simple processes $\{f_n\}_{n} \in \Mfam{0}{t}$ such that
	\begin{equation}
		\Exp{\int_0^t (f_n - f)^2 dt} \to 0.
	\end{equation}
	
	Then, the sequence $\{I(f_n)\}$ is a Cauchy sequence in $L_2\ProbSpace$ because
	\begin{align*}
		\Exp{(I(f_n) - I(f))^2} &= \Exp{\left|\int_0^t f_n dW - \int_0^t f dW\right|^2} \text{ (in $\ProbSpace$)} \\
								&= \Exp{\int_0^t(f_n - f)^2 ds} \text{ (in $([0,t]\times)\Omega,\mathscr B{([0,t])}\times\salgF, \lambda\times\Pm $)}.
	\end{align*}
	
	Therefore, there exists a random variable $I(f)\in L_2$ such that $I(f_n) \to I(f)$ in $L_2$. This limit is known as the \textbf{It\^o Integral}. We write:
	\begin{equation}
		I(f) := \int_0^t f(t, \omega) dW_t(\omega) = \lim_{n\to\infty}\int_0^t f_n(t,\omega) dW_t(\omega) \text{ (in $L_2$)}.
	\end{equation}
\end{definition}

\begin{proposition}[\textbf{Properties for $f$, $g$ $\in \Nfam{0}{t}$}]
	Let $f, g \in \Nfam{0}{t}$ then,
	\begin{equation}
		\Exp{\int_0^t f dW_s} = 0;
	\end{equation}
	
	\begin{equation}
		\Var{\int_0^t f dW_s} = \Exp{\left(\int_0^t f dW_s\right)} = \Exp{\int_0^t f^2 ds}\text{; and}
	\end{equation}
	
	\begin{equation}
		cov\left(\int_0^tfdW_s, \int_0^tgdW_s\right) = \int_0^t \Exp{f\cdot g} ds.
	\end{equation}
\end{proposition}

\begin{definition}[\textbf{Indefinte Integral}]
	Let $f\in \Nfam{0}{T}$, we define the indefinite It\^o integral
	\begin{equation}
		X(t) := \int_0^t f(s) dW_s \ \forall \ t\in[0, T].
	\end{equation}
\end{definition}

\begin{proposition}
	Let $f\in\Nfam{0}{T}$ and $X(t)$ an indefinite integral then,
	\begin{enumerate}
		\item $X(\cdot)$ is adapted to $\salgF$, i.e., $X(t)$ is $\salgF_t$-measurable for all $t\in[0, T]$; \label{it:adapted-int}
		\item $X(t)$ is an $L_2$ process and a $\salgF_t$-martingale \label{it:ito-martingale}
	\end{enumerate}
\end{proposition}

\begin{proof}
For $f\in\Mfam{0}{t}$, \ref{it:adapted-int} follows from the definition of an It\^o Integral:
\begin{equation}
  \int_0^t f(s) dW_s = \sum_{i=0}^{n-1} f(t_i) \Delta W_i;
\end{equation}
for $f\in\Nfam{0}{t}$, \ref{it:adapted-int} follows considering its limit in $L_2$.\\

To prove \ref{it:ito-martingale}, recall first that $\Exp{I(f)} = 0$ and $\Var{I(f)} = \int\Exp{f^2} dt$ then, $X(\cdot)$ is adapted, has mean zero, and variance $\int_0^t\Exp{f^2} dt$. We are left to show that
\begin{equation}
  \Exp{X(t) | \salgF_s} = X(s) \ \forall \ s \leq t.
\end{equation}

Noting that
\begin{align}
  X(t) &= \int_0^s f dW + \int_s^t f dW\\
  	&= X(s) + \int_s^t f dW,
\end{align}

proving that $\Exp{\int_s^t f dW | \salgF_s}= 0$ concludes the proof.\\

Suppose first that $f\in\Mfam{0}{T}$, it follows that
\begin{align}
	\Exp{\int_s^t fdW | \salgF_s}  &= \Exp{\sum_{i=0}^{n-1}f(t_i) \Delta W_i | \salgF_s}\nonumber\\
	&= \sum_{i=0}^{n-1}\Exp{f(t_i)\Delta W_i | \salgF_s}\label{eq:expectation_fi_salgs}.
\end{align}

Taking account of the fact that $\salgF_s \subseteq \salgF_{t_i}$ (take $S=t_0$) and considering (\ref{eq:expectation_fi_salgs}) we write
\begin{align}
  \Exp{f(t_i)\Delta W_i | \salgF_s} &= \Exp{\Exp{f(t_i)\Delta W_i | \salgF_{t_i}}| \salgF_s}\\
  &=\Exp{f(t_i)\Exp{\Delta W_i}| \salgF_s}\\
  &= 0
\end{align}

We generalize this result for $f\in\Nfam{0}{T}$ by means of the $L_2$ convergence.
\end{proof}


%TODO: Add reference to find the proof
\begin{theorem}
	Let $f\in\Nfam{0}{T}$. There exists a $t$-continuous version of
	\begin{equation}
		\int_0^t f(s,\omega) dW_s(\omega) \ 0\leq t \leq T.
	\end{equation}
I.e., there exists a $t$-continuous stochastic process $Y_t$ defined over $\ProbSpace$ such that
\begin{equation}
	\Pm\left[Y_t = \int_0^tf(s,\omega) dW_s(\omega)\right] = 1 \ \forall \ 0\leq t \leq T.
\end{equation}
\end{theorem}

\section{Extensions of It\^o's Integral}
%TODO: the following section is dedicated to define a more general family of ito processes that those defined for N[0, t] (we want to introduce the need of a new family of proceses not in L2, and the multidimensional ito integral ).
\begin{definition}\label{def:extended_f}
	Let $W(\cdot) := \{W_t : t\geq 0\}$ an n-dimensional Brownian motion process over $\ProbSpace$ and let $\mathscr{G}$, a family of $\salg$s where
	\begin{enumerate}
		\item $\mathscr{G}(s) \subseteq \mathscr{G}(t)$ for all $0\leq s \leq t$;
		\item For all $t, h > 0$, $W_{t+h} - W_t$ is independent of $\mathscr{G}(t)$.
	\end{enumerate}
	We define the family of processes $f\in\Nhatfam{0}{T}$
	\begin{enumerate}
		\item $(t, \omega) \to f(t, \omega):[0, \infty]\times\Omega \to \RNums$ is $\borelsalg\times\salgF$ measurable;
		\item $\int_0^t f^2(s) ds < \infty$ a.s.; and
		\item $f(\cdot, \omega)$ is independent of $\mathscr{G}(\cdot)$.
	\end{enumerate}
\end{definition}

\begin{definition}[\textbf{It\^o Integral for processes in $\Nhatfam{0}{T}$}]
	Consider the family of processes $f\in \Nhatfam{0}{T}$ as in definition \ref{def:extended_f}, we define the It\^o integral $\int_0^t fdW_t$  over a probability space $\ProbSpace$ as the limit (in probability) of of simple functions $f_n$ such that
		\begin{equation}
		\lim_{n\to \infty} \Pm\left(\left\{\omega: \int_0^t f_n(t,\omega)dW_t - \int_0^t f(t,\omega)dW_t\right\}>\epsilon \right) = 0
	\end{equation}
\end{definition}

\section{It\^o's Formula}

\begin{definition}[\textbf{It\^o Process}]\label{def:ito_process}
	let $W_t$ a standard Brownian motion on $\ProbSpace$. We define the It\^o process $X(t)$ over $\ProbSpace$ as
	\begin{equation}
		X(t) := \int_0^t\mu(s,\omega) ds  + \int_0^t\sigma(s,\omega) dW_s.
	\end{equation}
	
	Where
	\begin{enumerate}
		\item $\int_0^t \mu(s,\omega) ds < \infty$; and
		\item $\sigma(s,\omega) \in \Nhatfam{0}{T}$.
	\end{enumerate}
	
	The It\^o Process can also be defined in a differential form as follows:
	\begin{equation}
		dX(t) = \mu(t, \omega) dt + \sigma(t, \omega) \dW[t].
	\end{equation}
\end{definition}

\begin{theorem}[\textbf{It\^o's Formula}]\label{th:itos_formula}
	Let $X(t)$ be an It\^o process of the form
	\[
		dX(t) = \mu(t, \omega) dt + \sigma(t, \omega) dW_t,
	\]
	and $g(t, \omega) \in C^2([0, \infty]\times\RNums)$. Then,
	\begin{equation}
		Y(t) := g(t, X_t)
	\end{equation}
	is also an It\^o Process where
	
	\begin{equation}\label{eq:itos_formula}
		dY(t) = \partialwrt{g}{t}(t, X(t)) dt + \partialwrt{g}{x}(t, X(t)) \left(dX(t)\right) + \frac{1}{2}\partialwrt[2]{g}{x} (t, X(t)) \left(dX(t)\right)^2
	\end{equation}	
\end{theorem}

\begin{proposition}\label{prop:on_bounded_ito_g}
We may assume that $g$ and its two first derivatives are bounded in the following integral:
\begin{equation}\label{eq:g_bounded}
  g(0, X(0)) + \int_0^t \left(\partialwrt{g}{s} + \mu\partialwrt{g}{s} + \sigma^2\partialwrt[2]{g}{x}\right) ds + \int_0^t\sigma^2(s, \omega) dW_s
\end{equation}

Where $X(s) = \int_0^t \mu(s, \omega) ds + \int_0^t \sigma(s, \omega) dW_s$.

\begin{proof}
	For fixed $t\geq 0$ and $n\in\mathbb{N}$ we define $g_n(s, x) = g(s, x)$ for all $s \leq t$, $|x|\leq n$. For fixed $\omega$ define $\tau_n(\omega) = \tau_n := \inf\{s>0: |X(s,\omega)| \geq n\}$, and
	
	\begin{equation}\label{eq:equivalence_stop_time_integral}
		\int_0^{t} \sigma(s,\omega) \partialwrt{g_n}{x}(s, X(s)) \ind_{s\leq \tau_n} dW_s = \int_0^{t\wedge\tau_n} \sigma(s,\omega) \partialwrt{g_n}{x} (s, X(s)) dW_s.
	\end{equation}

Note that \ref{eq:equivalence_stop_time_integral} takes values up to $\min\{t, \tau_n\}$, which are the bounds defined for $g_n$ (it would be zero otherwise) then,

	\begin{equation}
	  \int_0^{t\wedge\tau_n} \sigma(s,\omega) \partialwrt{g_n}{x} (s, X(s)) dW_s = \int_0^{t\wedge\tau_n} \sigma(s,\omega) \partialwrt{g}{x} (s, X(s)) dW_s.
	\end{equation}
	

It follows from (\ref{eq:g_bounded}) that $g(t\wedge\tau_n, X(t\wedge\tau_n))$ equals
\begin{equation}
  g(0, X(0)) + \int_0^{t\wedge\tau_n} \left(\partialwrt{g}{s} + \mu\partialwrt{g}{s} + \sigma^2\partialwrt[2]{g}{x}\right) ds + \int_0^{t\wedge\tau_n}\sigma^2(s, \omega) dW_s
\end{equation}

Finally,

\begin{equation}
  \Pm(\tau_n \geq t) = 	\Pm(\inf	\{s: |X_s| \leq n\} > t) \to 1
\end{equation}

Which shows that $g$ is bounded for all $n \geq 1$.
\end{proof}
\end{proposition}

\begin{proof}[proof of It\^o's formula]
	In order to prove \ref{th:itos_formula} we will consider another representation of It\^o's formula replacing $dX(t) = \mu dt + \sigma dW_t$ in (\ref{eq:itos_formula}). It then follows that
	
	\begin{align*}
		&\partialwrt{g}{t}(t, X(t)) dt + \partialwrt{g}{x}(t, X(t)) \left(dX(t)\right) + \frac{1}{2}\partialwrt[2]{g}{x} (t, X(t)) \left(dX(t)\right)^2 \\
		&= \partialwrt{g}{t}(t, X(t)) dt + \partialwrt{g}{x}(t, X(t)) \left(\mu dt + \sigma dW_t\right) + \\
		&\phantom{{}=1} \frac{1}{2}\partialwrt[2]{g}{x} (t, X(t)) \left(\mu dt + \sigma dW_t\right)^2.
	\end{align*}
	
	Rearranging terms and integrating both sides, 
	\begin{equation}
  Y(t) = \int_0^t \left(\partialwrt{g}{s}{(s, X_s)} + \mu\partialwrt{g}{x}{(s, X_s)} + \frac{\sigma^2}{2}\partialwrt[2]{g}{x}{(s, X_s)}\right)ds + \int_0^t \sigma dW_t.
	\end{equation}

	Let us take into account $g_n \in C^2([0, \infty]\times\RNums)$ such that $g_n$ and its two first derivates are bounded for all $n$, and converge uniformly to $g$, $\partialwrt{g}{t}$, $\partialwrt{g}{x}$, and $\partialwrt[2]{g}{t}$. By \ref{prop:on_bounded_ito_g}, we can assume that such sequence of functions exists.\\
	
	Taking $g(t, X(t))$ and expanding as a Taylor series we see that:
	\begin{equation}\label{eq:taylor_g}
		g(t, X(t)) = g(0, X(0)) + \sum_{i=0}^{n-1}\Delta g(t_i, X_i);
	\end{equation}
	
	where 
	\[
		\Delta g(t_i, X_i) = \partialwrt{g}{t}\Delta t_i + \partialwrt{g}{t} \Delta X_i + \frac{1}{2}\partialwrt[2]{g}{t} (\Delta t_i)^2 + \frac{\partial^2 g}{\partial x \partial y}(\Delta t_i)(\Delta X_i) + R_i
	\]
	
	We note that, as $\Delta t_i \to 0$,
\begin{align} 
\partialwrt{g}{t} \Delta t_i &\to \partialwrt{g}{t} dt
\end{align}

\begin{align} 
\partialwrt{g}{x} \Delta X_i &= \partialwrt{g}{x}\left(\mu\Delta t_i + \sigma \Delta W_i \right) \nonumber \\
&\to  \partialwrt{g}{x}(\mu dt + \sigma dW_t)
\end{align}

\begin{align}
\frac{1}{2}\partialwrt[2]{g}{x} (\Delta X_i)^2 &= \frac{1}{2}\partialwrt[2]{g}{x} (\mu\Delta t_i + \sigma \Delta W_i)^2 \nonumber\\
&= \frac{1}{2}\partialwrt[2]{g}{x} (\mu^2 (\Delta t_i)^2 + 2\sigma\mu \Delta W_i \Delta t_i + \sigma^2 (\Delta W_i)^2) \nonumber\\
&\to \frac{1}{2} \sigma^2 dt
\end{align}

Finally, 
\begin{equation}
  R_i = \bigO((\Delta t_i)^3 + (\Delta t_i)^2(\Delta X_i) + (\Delta t_i)(\Delta X_i)^2 + (\Delta X_i)^3).
\end{equation}

Expanding $\Delta X_i$ and cancelling out terms we find out that

\begin{align}
  R_i &= \bigO((\Delta t_i)^3 + (\Delta t_i)^2(\Delta W_i)) + (\Delta t_i)(\Delta W_i))^2 + (\Delta W_i)^3) \nonumber \\
  	&= \bigO((\Delta t_i)^3 + (\Delta t_i)^2(\Delta W_i)) + (\Delta t_i)(\Delta t_i) + (\Delta W_i)^2(\Delta W_i))\nonumber \\
  	&= \bigO((\Delta t_i)^2(\Delta t_i + \Delta W_i + 1) + \Delta t_i \Delta W_i) \label{eq:residual_ito_formula}.
\end{align}

We note that $(\Delta t_i)^2 \to 0$ and $(\Delta t_i)(\Delta W_i) \to 0$, thus (\ref{eq:residual_ito_formula}) goes to 0.\\

We conclude that the limit (in $L_2$) of (\ref{eq:taylor_g}) is

\begin{align}
	g(t, X(t)) &= g(0, X_0) + \int_0^t\left( \partialwrt{g}{s}ds + \partialwrt{g}{x}{\mu ds} + \partialwrt{g}{x}{\sigma dW_s} + \frac{1}{2}\partialwrt[2]{g}{s}{\sigma^2 ds} \right)\\
			 &= g(0, X_0) + \int_0^t\left(\partialwrt{g}{s} + \mu \partialwrt{g}{x} +\sigma^2 \partialwrt[2]{g}{s} \right)ds + \int_0^t\sigma \partialwrt{g}{x}{dW_s}.
\end{align}
\end{proof}

\begin{definition}[\textbf{General $n$ It\^o Process}]
	Let $W_t(\omega)$ be an $n$-dimensional Brownian motion, and $\{\mu_i\}_{i\in\{1,\ldots,n\}}$, $\{\sigma_{ij}\}_{i\in\{1,\ldots,n\}, j\in\{1,\ldots,m\}}$ satisfying conditions 1 and 2 as defined on \ref{def:ito_process}. We define the \textit{general it\^o process} as the collection of $X(t) = \{X^{(1)}(t), \ldots, X^{(n)}(t)\}$ processes where
	\begin{equation}
		dX^{(k)} = \mu_k dt + \sum_{j=1}^{m}\sigma_{kj} dW_j.
	\end{equation}

If we represent $\mu$ as a matrix ($n\times 1$) of elements $m_i$; $\sigma$ a matrix ($n\times m$) matrix of elements $\sigma_{ij}$. We represent the general It\^o Process as
\begin{equation}
  dX_t := dX(t) = \mu dt + \sigma dW_t.
\end{equation}
\end{definition}
	
\begin{theorem}[\textbf{The General It\^o Formula}]\label{th:general_ito_formula}
	Let $dX_t = \mu dt + \sigma dW_t $ be an $n$-dimensional It\^o process and $g:[0,\infty)\times\RNums^n\to\RNums^n$ then, $g(t, X_t) =: Y(t)$ is again an It\^o process whose $k$-th entry is given by
	\begin{equation}
		dY^{(k)} = \partialwrt{g_k}{t}{(t, X)} dt + \sum_i \partialwrt{g_k}{x_i}{(t, X)}(dX_i) + \frac{1}{2}\sum_{i,j}\frac{\partial^2g_k}{\partial x_i\partial x_j} (dX_i)(dX_j).\label{eq:general_ito_formula}
	\end{equation}
\end{theorem}

As an example of \ref{th:general_ito_formula} we consider $X(t)$, $Y(t)$ It\^o processes in $\RNums$ where

\begin{align}
	dX_t &= \mu_1 dt + \sigma_1 dW^{(1)}_t\nonumber, \text{ and}\\
	dY_t &= \mu_2 dt + \sigma_2 dW^{(2)}_t\nonumber.
\end{align}
 and $g(x,y) = xy$. What is the dynamic of $g(X_t, Y_t)$?\\

If we expand (\ref{eq:general_ito_formula}) for $X_t$ and $Y_t$ we have
\begin{align}
  d(X_t, Y_t) &= \partialwrt{g}{t}{} dt + \left(\partialwrt{g}{x} dX_t + \partialwrt{g}{y} dY_t \right) + \nonumber \\
  & \frac{1}{2}\left(2\frac{\partial^2g_k}{\partial x\partial y} dX dY + \partialwrt[2]{g}{x} (dX_t)^2 + \partialwrt[2]{g}{y} (dY_t)^2 \right). \label{eqproof:ito_prod_formula}
\end{align}

Where $\partialwrt{g}{t} = 0$, $\partialwrt{g}{x} = Y_t$, $\partialwrt{g}{y} = X_t$, $\frac{\partial^2g_k}{\partial x\partial y} = 1$, $\partialwrt[2]{g}{x} = 0$, and $\partialwrt[2]{g}{y} = 0$. We rewrite (\ref{eqproof:ito_prod_formula}) as

\begin{equation}
  d(X_t, Y_t) = X_t dY_t + Y_t dX_t + dX_t dY_t. \label{eq:ito_rule_product}
\end{equation}

\section{Stochastic Differential Equations}
Suppose $S_t$ is the price of a stock at time $t$, then $dS_t$ measures the change in the value of the stock at around $t$ (an infinitesimal-time interval). Given that the change in the value of the asset is

\begin{equation}
	\frac{dS_t}{S_t} = \mu dt + \sigma dW_t, \label{eq:stock_dynamic}
\end{equation}

What is $S_t$ that satisfies (\ref{eq:stock_dynamic})?\\

Consider $g(t, x)= \log x$ and $dS_t = S_t(\mu dt + \sigma dW_t)$, we see that
\begin{align}
	\left(dS_t\right)^2 &= (S_t)^2(\mu dt + \sigma dW_t)^2\nonumber\\
		&= (S_t)^2(\mu^2 \left(dt\right)^2 + \mu\sigma (dtdW_t) + \sigma^2 (dW_t)^2)\nonumber\\
		&= S_t^2\sigma^2 dt
\end{align}

We solve $d(\log S_t)$ using \ref{th:itos_formula}.
\begin{align}
	d(\log S_t) &= \partialwrt{g}{t}{(t, S_t)} dS_t + \partialwrt{g}{x}{(t, S_t)} dS_t + \partialwrt[2]{g}{x}{(t, S_t)} (dS_t)^2 \nonumber \\
&= \frac{1}{S_t}\left(S_t(\mu dt + \sigma dW_t)\right) + \frac{1}{2}\left(-\frac{1}{S_t^2} \right)S_t^2\sigma^2 dt \nonumber \\
&= \mu dt + \sigma dW_t - \frac{1}{2}\sigma^2 dt \nonumber \\
&= \left(\mu - \frac{1}{2}\sigma^2\right)dt + \sigma dW_t. \label{eqproof:stock_dynamic}
\end{align}

Integrating both sides of (\ref{eqproof:stock_dynamic}) it follows that
\begin{equation}
	\log\left(\frac{S_t}{S_0}\right) = \left(\mu - \frac{1}{2}\sigma^2\right) t + \sigma\int_0^t dW_s.
\end{equation}

Therefore,
\begin{equation}
  S_t = S_0e^{\left(\mu - \frac{1}{2}\sigma^2\right) t + \sigma\int_0^t dW_s}.
\end{equation}

\begin{figure}[h!]
\centering
  \includegraphics[width=0.9\textwidth]{../images/asset_sde}
  \caption{SDE with $\sigma=0$ and $\sigma=0.2$}
\end{figure}


\begin{theorem}[\textbf{Feynman-Kac}] \label{th:feynman-kac}
	Consider the following partial differential equation
	\begin{equation}
		\partialwrt{C}{t}{(t,x)}	 + \mu(t, x)\partialwrt{C}{x}{(t,x)} + \frac{1}{2}\sigma^2(t,x)\partialwrt[2]{C}{x}{(t,x)}  - V(t,x)C(t,x) + f(x,t) = 0 \label{eq:feynman_kac_pde}.
	\end{equation}
	Defined for $x\in\RNums$, $t\in[0,T]$ and subject to the final boundary condition $C(T, X_T) = P(X_T)$. Then, $C(x,t)$ can be written as the following conditional expectation
	\begin{equation}
	C(t, x) = \Exp{\int_t^Te^{-\int_t^r V(X_\tau, \tau) d\tau} f(X_r, r) dr + e^{-\int_t^TV(\tau, X_\tau) d\tau} P(X_T)| \salgF_t}.
	\end{equation}

Where $X_t$ is the It\^o process $dX_t = \mu(t,x) dt + \sigma(t, X_t) dW_t$.
\end{theorem}

\begin{proof}
Let
\begin{equation}
	Y_s = e^{-\int_t^s V(\tau, X_\tau) d\tau}C(X_s, s) + \int_t^s e^{-\int_t^rV(\tau, X_\tau)}f(X_r, r) dr.
\end{equation}

Applying It\^o's product rule (\ref{eq:ito_rule_product}) we get
\begin{align}
	dY_s &= d\left(e^{-\int_t^s V(\tau, X_\tau) d\tau}C(X_s, s) + \int_t^s e^{-\int_t^rV(\tau, X_\tau)}f(X_r, r) dr\right) \nonumber \\
	&= d\left(e^{-\int_t^s V(\tau, X_\tau) d\tau}\right)C(X_s, s) + e^{-\int_t^s V(\tau, X_\tau) d\tau}d\left(C(X_s, s)\right) + \nonumber \\
	&\phantom{{}=1} d\left(e^{-\int_t^s V(\tau, X_\tau) d\tau}\right)d\left(C(X_s, s)\right) + d\left(\int_t^s e^{-\int_t^rV(\tau, X_\tau)}f(X_r, r) dr\right) \label{eqproof:feynman_kac_Ys1}.
\end{align}

Where we note
\begin{equation}
  d\left(e^{-\int_t^s V(\tau, X_\tau) d\tau}\right) = e^{-\int_t^s V(\tau, X_\tau) d\tau}\left(-V(s, X_s) ds\right).
\end{equation}

It follows that 
\begin{equation}
    d\left(e^{-\int_t^s V(\tau, X_\tau) d\tau}\right)d\left(C(s, X_s)\right) = 0, 
\end{equation}
and
\begin{equation}
  d\left(\int_t^s e^{-\int_t^rV(\tau, X_\tau)}f(X_r, r) dr\right) = e^{-\int_t^rV(\tau, X_\tau)}f(X_s, s) ds.
\end{equation}

We rewrite (\ref{eqproof:feynman_kac_Ys1}) as
\begin{align}
  dY_s &= -V(s, X_s)e^{-\int_t^s V(\tau, X_\tau) d\tau}C(s, X_s) ds + e^{-\int_s^t V(\tau, X_\tau)d\tau}d(C(s, X_s))\nonumber +\\
   &\phantom{{}=1}e^{-\int_t^s V(\tau, X_\tau)d\tau}f(s, X_s) ds\label{eqproof:feynman_kac_Ys2}.
\end{align}

Applying It\^o's formula to $d(C(s, X_s))$,
\begin{align}
  d(C(s, X_s)) &= \partialwrt{C}{s}{(s, X_s)} + \mu\partialwrt{C}{x}{(s, X_s)}(dX_s) + \frac{1}{2}\sigma^2\partialwrt{C}{x}{(s, X_s)} (dX_s) ^ 2\nonumber\\
  	&= \partialwrt{C}{s}{(s, X_s)} + \mu\partialwrt{C}{x}{(s, X_s)}(\mu ds + \sigma dW_s) + \frac{1}{2}\sigma^2\partialwrt{C}{x}{(s, X_s)} \sigma^2 dt,
\end{align}

we expand (\ref{eqproof:feynman_kac_Ys2}) 
\begin{align}
  dY_s &= -V(s, X_s)e^{-\int_t^s V(\tau, X_\tau) d\tau}C(s, X_s) ds + \nonumber \\
   &\phantom{{}=1} \partialwrt{C}{s}{(s, X_s)} + \mu\partialwrt{C}{x}{(s, X_s)}(\mu ds + \sigma dW_s) + \frac{1}{2}\sigma^2\partialwrt{C}{x}{(s, X_s)} \sigma^2 dt\nonumber + \\
   &\phantom{{}=1} e^{-\int_t^s V(\tau, X_\tau)d\tau}f(s, X_s) ds\label{eqproof:feynman_kac_Ys3}.
\end{align}

Rearranging terms in (\ref{eqproof:feynman_kac_Ys3}) and considering (\ref{eq:feynman_kac_pde}),
\begin{equation}
	dY_s =e^{-\int_t^s V(\tau, X_\tau) d\tau} \sigma \partialwrt{C}{x}{(s, X_s)}dW_s \label{eqproof:feynman_kac_Ys4}.
\end{equation}

Integrating both sides of (\ref{eqproof:feynman_kac_Ys4}),
\begin{equation}
  \int_t^T dY_s =\int_t^T e^{-\int_t^s V(\tau, X_\tau) d\tau} \sigma \partialwrt{C}{x}{(s, X_s)}dW_s ds,
\end{equation}

and denoting $h(X_s) = e^{-\int_t^s V(\tau, X_\tau) d\tau} \sigma \partialwrt{C}{x}{(s, X_s)}$ we conclude that it is an It\^o integral which implies that taking its expectation conditioned up to $t$ yields
\begin{equation}
  \Exp{Y_T - Y_t|\salgF_t} = \Exp{h(X_\tau) dW_t|\salgF_t} = 0.
\end{equation}
It follows
\begin{align}
	\Exp{Y_T|\salgF_t} &= \Exp{Y_t|\salgF_t}\nonumber\\
	&= \Exp{e^{-\int_t^t V(\tau, X_\tau) d\tau}C(t, X_t) + \int_t^t e^{-\int_t^rV(\tau, X_\tau)}f(X_r, r) dr|\salgF_t}\nonumber\\
	&= \Exp{C(t, X_t)|\salgF_t}\nonumber\\
	&= C(t, X_t).\label{eqproof:feynman_kac_exp}
\end{align}

Noting the final boundary condition and (\ref{eqproof:feynman_kac_exp}),
\begin{align}
  C(t, X_t) &= \Exp{Y_T|\salgF_t} \nonumber\\
  	&= \Exp{e^{-\int_t^T V(\tau, X_\tau) d\tau}C(T, X_T) + \int_t^T e^{-\int_t^rV(\tau, X_\tau)}f(X_r, r) dr|\salgF_t}\nonumber\\
  	&= \Exp{e^{-\int_t^T V(\tau, X_\tau) d\tau}P(X_T) + \int_t^T e^{-\int_t^rV(\tau, X_\tau)}f(X_r, r) dr|\salgF_t}\nonumber.
\end{align}
\end{proof}


\section{Girsanov}
\begin{theorem}[\textbf{Lévy's characterization of Brownian Motion}]\label{th:levy_brownian motion}
	let $W_t$ be a one dimensional Brownian Motion adapted to $\salgF_t$. Then $(W_t^2 - t)$ is a martingale w.r.t. $\salgF$.
	The converse is also true: if $\ctspr$ is any continuous time stochastic process adapted to $\salgF_t$ such that
	\begin{itemize}
		\item $X_0 = 0$;
		\item $X_t$ is a martingale w.r.t. $\salgF_t$; and
		\item $(X_t^2 - t)$ is a martingale w.r.t. $\salgF_t$.
	\end{itemize}
	
	Then $X_t$ is a Brownian motion.
\end{theorem}

\begin{proposition}\label{prop:expectation_two_measures}
	Let $\mu$ and $\lambda$ be two probability measures defined over $(\Omega, \salgF)$ where $d\lambda(\omega) = f(\omega)d\mu(\omega)$ for some $f\in L_1$. If $X$ is a random variable over $(\Omega, \salgF)$ such that
	\begin{equation}
		\ExpMeasure{\lambda}{|X|} = \int_\Omega |X(\omega)| f(\omega) d\mu(\omega) < \infty,
	\end{equation}
	and $\mathscr{H}\subseteq\salgF$ a $\salg$. Then,
	\begin{equation}
	  \ExpMeasure{\lambda}{X|\mathscr{H}} \cdot \ExpMeasure{\mu}{f|\mathscr{H}} = \ExpMeasure{\mu}{fX|\mathscr H} \text{ a.s.}
	\end{equation}
\end{proposition}

\begin{proof}
	Let $H\in\mathscr{H}$. By definition of the conditional expectation,
	\begin{align}
		\int_H \ExpMeasure{\lambda}{X|\mathscr H} fd\mu &= \int_H \ExpMeasure{\lambda}{X|\mathscr H} d\lambda\nonumber\\
		&= \int_H X d\lambda\nonumber \\
		&= \int_H (Xf)d\mu\nonumber \\
		&= \int_H \ExpMeasure{\lambda}{Xf|\mathscr H} d\mu. \label{eqproof:expectation_two_measures1}
	\end{align}
	
On the other hand,
\begin{align}
	\int_H \ExpMeasure{\lambda}{X|\salgH} fd\mu &= \int_\Omega \ExpMeasure{\lambda}{X|\salgH}\ind_H f d\mu\nonumber\\
	&= \ExpMeasure{\mu}{\ExpMeasure{\lambda}{X|\salgH}\ind_H f}\nonumber\\
	&= \ExpMeasure{\mu}{\ExpMeasure{\mu}{\ExpMeasure{\lambda}{X|\salgH}\ind_H f | \salgH}}\nonumber\\
	&= \ExpMeasure{\mu}{\ind_H\ExpMeasure{\lambda}{X|\salgH} \ExpMeasure{\mu}{f | \salgH}}\nonumber\\
	&= \int_H \ExpMeasure{\lambda}{X|\salgH} \ExpMeasure{\mu}{f | \salgH} d\mu.\label{eqproof:expectation_two_measures2}
\end{align}

If follows from (\ref{eqproof:expectation_two_measures1}) and (\ref{eqproof:expectation_two_measures2}) that
\begin{equation}
  \int_H \ExpMeasure{\lambda}{Xf|\mathscr H} d\mu = \int_H \ExpMeasure{\lambda}{X|\salgH} \ExpMeasure{\mu}{f | \salgH} d\mu.
\end{equation}
\end{proof}
\begin{theorem}[\textbf{Girsanov Theorem v.1}]
	Let $Y_t$ be a one dimensional It\^o process:
	\[
		dY_t = a(t, \omega) dt + dW_t(\omega); \ t \leq T, Y_0 = 0.
	\]

Set 
\[
	M_t = e^{-\int_0^t a(s,\omega) dW_s - \frac{1}{2}\int_0^t a^2(s,\omega) ds},
\]

% TODO: Prove that the following condition guarantees that the last equation is a martingale.
such that
\[
	\Exp{e^{\frac{1}{2}\int_0^t a^2(s,\omega) ds}} < \infty.
\]

Define the measure $\Qm$ on $(\Omega,\salgF)$ by $d\Qm(\omega) = M_t d\Pm(\omega)$
Then, $Y_t$ is a 1-dimensional Brownian motion w.r.t. the measure $\Qm$ defined over $(\Omega,\salgF)$ where,
\[
	d\Qm(\omega) = M_t d\Pm(\omega).
\]
\end{theorem}

\begin{proof}
By \ref{th:levy_brownian motion}, to show that $Y_t$ is indeed a Brownian motion, it must be the case that
\begin{enumerate}
	\item $Y_t$ is a martingale w.r.t. $\Qm$; and \label{eqproof:girsv1_1}
	\item $Y_t^2 - t$ is a martingale w.r.t. $\Qm$. \label{eqproof:girsv1_2}
\end{enumerate}

We will assume that $a(s,\omega)$ is bounded for all $s\in[0,T]$ and $\omega\in\Omega$. To show \ref{eqproof:girsv1_1} we denote $K_t = M_t Y_t$, we consider
\[
	dY_t = a_t, dt + dW_t,
\]

and compute $dM_t$, $M_t = e^{X_t}$; $X_t = -\int_0^t a(s,\omega) dW_s - \frac{1}{2}\int_0^t a^2(s,\omega) ds$. Where
\begin{equation}
  dX_t = -a_t dW_t - \frac{1}{2}a^2_t dt \iff -a_tdW_t = \frac{1}{2}a^2_tdt + dX_t.
\end{equation}

It follows that, for $g(t, x) = e^x$,
\begin{align}
	dM_t = dg(X_t) &= \partialwrt{g}{t}{(t, X_t)} dt + \partialwrt{g}{x}{(t, X_t)} dX_t + \frac{1}{2}\partialwrt[2]{g}{x}{t, X_t} (dX_t)^2. \nonumber \\
	&= e^{X_t} (dX_t) + \frac{1}{2}e^{X_t}(a^2_t dt) \nonumber \\
	&= e^{X_t}(dX_t + \frac{1}{2}a^2_t dt) \nonumber \\
	&= -M_ta_tdW_t.
\end{align}

Now,
\begin{align}
	dK_t &= d(M_tY_t)\nonumber \\
		&= Y_t(dM_t) + M_t(dY_t) + (dM_t)(dY_t). \nonumber\\
		&= Y_t(-M_ta_tdW_t) + + M_t(a_t, dt + dW_t) + (-M_ta_tdW_t))(a_t, dt + dW_t)\nonumber \\
		&= M_t(-Y_t a_t dW_t + a_t dt + dW_t - a^2_tdW_tdt - a_t dt) \nonumber \\
		&= M_t(1 - Y_t a_t)dW_t.
\end{align}

Since $dK_t$ has no drift, it proves that $M_tY_t$ is a martingale w.r.t. $\Pm$.

To conclude that $Y_t$ is a martingale w.r.t. $\Qm$, let $0 \leq t \leq s \leq T$. By \ref{prop:expectation_two_measures},

\[
	\ExpMeasure{\Qm}{Y_s|\salgF_t} \cdot \ExpMeasure{\Pm}{M_s|\salgF_t} = \ExpMeasure{\Pm}{M_sY_s|\salgF_t}
\]

Then,
\begin{align}
	\ExpMeasure{\Qm}{Y_s | \salgF_t} &= \ExpMeasure{\Pm}{M_sY_s|\salgF_t} / \ExpMeasure{\Pm}{M_s|\salgF_t} \nonumber\\
	&=\ExpMeasure{\Pm}{K_s|\salgF_t} / \ExpMeasure{\Pm}{M_s|\salgF_t} \nonumber\\
	&= K_t / M_t \nonumber \\
	&= Y_t. \label{eqproof:girsv1_3}
\end{align}

Which concludes the first part of the proof: $Y_t$ is a Brownian motion w.r.t. $\Qm$.\\

To show \ref{eqproof:girsv1_2}, denote $R_t = M_tG_t(\omega)$; $G_t(\omega) := Y_t^2 -t$; and $g(t,x) = x^2- t$. As in the first, part of the proof, we will first prove that $R_t$ is a martingale w.r.t. $\Pm$, and will conclude that $G_t$ is a martingale w.r.t. $\Qm$. To do so, we see that
\begin{align}
	dG(t,\omega) &= \partialwrt{g}{t}{(t,Y_t)} dt + \partialwrt{g}{x}{(t,Y_t)} (dY_t) + \frac{1}{2}\partialwrt{g}{x}{(t,Y_t)}(dY_t)^2\nonumber\\
	&= -dt + 2Y_t dY_t + (dY_t) ^2 \nonumber \\
	&= -dt + 2Y_t dY_t + dt \nonumber \\
	&= 2Y_t(a_t, dt + dW_t).
\end{align}

Now,
\begin{align}
	dR_t &= M_t (dG_t) + G_t (dM_t) + (dG_t)(dM_t)\nonumber\\
		&= M_t(2Y_t[a_t, dt + dW_t]) + G_t(-M_ta_tdW_t) + (2Y_t[a_t, dt + dW_t])(-M_ta_tdW_t) \nonumber\\
		&= 2Y_tM_ta_tdt + 2M_tY_tdW_t - G_tM_ta_tdW_t - 2Y_tM_ta_tdt \nonumber\\
		&= M_t(wY_t - G_ta_t)dW_t.
\end{align}

Noting that $R_t$ is a martingale, 
\begin{equation}
	\ExpMeasure{\Qm}{G_s | \salgF_t} \ExpMeasure{\Pm}{M_s|\salgF_t} = \ExpMeasure{\Pm}{G_sM_s|\salgF_t}.
\end{equation}
Then,
\begin{align}
	\ExpMeasure{\Qm}{G_s | \salgF_t} &= \ExpMeasure{\Pm}{G_sM_s|\salgF_t} / \ExpMeasure{\Pm}{M_s|\salgF_t}\nonumber\\
	&= \ExpMeasure{\Pm}{R_s|\salgF_t} / \ExpMeasure{\Pm}{M_s|\salgF_t}\nonumber\\
	&= G_s.
\end{align}
\end{proof}

\begin{theorem}[\textbf{Girsanov Theorem V.2.}]
	Let $Y_t \in R^n$ be an It\^o process of the form
	\begin{equation}
		dY_t = \beta(t,\omega) dt + \theta(t,\omega) dW_t
	\end{equation}
Where $\beta \in \RNums^{n}$, $\theta \in \RNums^{n\times m}$, and $W_t$ is an $n$ dimensional brownian motion.\\

Suppose there exists a process $u(t,\omega), \alpha(t, \omega) \in \Nhatfam{0}{T}$ such that
\[
	\theta(t,\omega) u(t,\omega) = \beta(t, \omega) - \alpha(t, \omega),
\]

and 
\[
	\Exp{e^{\frac{1}{2}\int_0^T u^2(t,\omega) dt}} < \infty.
\]

Denote
\begin{equation}
	M_t(\omega) = e^{-\left(\int_0^t u(s,\omega) dW_s + \frac{1}{2}\int_0^t u^2(s, \omega) ds\right)},
\end{equation}
and $d\Qm = M_t d\Pm$ over $\salgF_t$. Then,
\begin{equation}
  \hat W_t := \int_0^t u(s,\omega) ds + W_t
\end{equation}
is a Brownian motion w.r.t. $\Qm$ and, in terms of $\hat W_t$, $Y_t$ has the stochastic integral representation

\begin{equation}
	dY_t(\omega) = \alpha(t,\omega) dt + \theta(t, \omega) d\hat W_t.	
\end{equation}
\end{theorem}

\begin{proof}
	By the Girsanov Theorem V.1, $\hat W_t$ is a Brownian motion w.r.t. $\Qm$ considering
	\begin{itemize}
		\item $\hat W_t = \int_0^t u(s,\omega) ds + W_t$, then $dW_t = d\hat W_t - u(t,\omega) dt$;
		\item $dY_t = \beta(t,\omega) dt + \theta(t,\omega) dW_t$; and
		\item $\alpha(t,\omega) = \beta(t,\omega) - \theta(t,\omega) u(t,\omega)$.
	\end{itemize}	
Then,
\begin{align}
	dY_t &= \beta(t,\omega) dt + \theta(t,\omega) dW_t\nonumber\\
	&= \beta(t,\omega) dt + \theta(t,\omega)[\hat W_t - u(t,\omega) dt]\nonumber\\
	&= [\beta(t,\omega) - \theta(t,\omega)u(t,\omega)]dt + \theta(t,\omega)\hat W_t\nonumber\\
	&= \alpha(t,\omega) dt + \theta(t,\omega)\hat W_t
\end{align}
\end{proof}
\end{document}