\documentclass[../TGMAFFIRO.tex]{subfiles}

\begin{document}
\section{It\^o's Integral}
Let $\ProbSpace$ a probability space and consider the spaces $L_2\ProbSpace$ and $L_2([a, b]\times\Omega, \mathscr{B}\otimes\salgF)$ with inner product
\begin{equation}
  \innerprod{f}{g} := \int_\Omega\int_0^t f(s) g(s) ds d\Pm = \Exp{\int_0^t f(s)g(s) ds},
\end{equation}


and norm in $L_2([a, b] \times \Omega)$
\begin{equation}
  \norm{f}_{[a, b] \times \Omega} := \sqrt{\innerprod{f}{f}} = \sqrt{\Exp{\int_0^t f(s)g(s) ds}}.
\end{equation}

We want to make sense of
\begin{equation}
  \int_a^bf(t,\omega) \dW[t](\omega).
\end{equation}

Where $W_t$ is a standard Brownian motion and $f$ belongs to a special family of functions denoted by $\Nfam{a}{b}$.

\begin{definition}
	Let $0\leq a\leq b$. Denote $\Nfam{a}{b}$ the family of stochastic process such that
	\begin{enumerate}
		\item $(t, \omega) \to f(t, \omega):[0, \infty]\times\Omega \to \RNums$ is $\borelsalg\times\salgF$ measurable;
		\item $f(t,\omega)$ is $\salgF_t$-adapted; and
		\item $f\in L_2([a,b]\times\Omega)$ i.e. $\int_a^b\int_\Omega f^2 d\Pm dt = \int_a^b\Exp{f^2}dt = \Exp{\int_a^b f^2 dt} < \infty$.
	\end{enumerate}
\end{definition}

\begin{definition}[\textbf{Simple Process}]
	We define the process $f\in \Nfam{a}{b}$ as simple if there exists a partition $\{t_i\}_{i=0}^n$ where $t_0 = a$, $t_n = b$, and $t_i < t_j \ \forall \ i < j$, such that $f$ can be represented as a lineal combination of the form
	\begin{equation}
		f(t) = \sum_{i=0}^{n-1}f(t_i)\ind_{[t_i, t_{i+1}]}(t).
	\end{equation}
\end{definition}

\begin{definition}
	We define the family of processes $f$ in $\Mfam{a}{b}$ that are processes in $\Nfam{a}{b}$ and can be written as simple processes. 
\end{definition}

\begin{definition}
	Let $f\in\Mfam{a}{b}$, we define the It\^o integral of $f$ as
\begin{equation}
	  \int_a^b f(t) dW_t := \sum_{i=0}^{n-1} \DeltaW .
\end{equation}

With $\Delta W_i := W_{t_{i+1}} - W_{t_{i}}$.\\
We will also denote It\^o's integral as $I(f)$ or $\int f$.
\end{definition}

\begin{proposition}
	Let $f$, $g$ $\in \Mfam{0}{t}$ then
	\begin{enumerate}
		\item $I(\alpha f + \beta g) = \alpha I(f) + \beta I(g)$;
		\item $\Exp{I(f)} = 0$; and
		\item $\Var{I(f)} = \Exp{\left(\int_0^t f(s) dW_s\right)^2} = \Exp{\int_0^t f^2(t) dt}$ (It\^o Isometry).
	\end{enumerate}
\end{proposition}

\begin{proof}
	1). Let $f,g\in\Mfam{0}{t}$, and $\alpha, \beta \in\RNums$. Then, we can write $f$ and $g$ as a simple process. Namely,
	\begin{align}
		f(t) = \sum_{i=0}^{n-1}f(t_i)\ind_{i}(t) \text{; and}\\
		g(t) = \sum_{j=0}^{m-1}g(t_j)\ind_{j}(t).
	\end{align}
	\newcommand{\fsimple}{\sum_{i=0}^{n-1}f(t_i)\ind_{i}(t)}
	\newcommand{\gsimple}{\sum_{j=0}^{m-1}g(t_j)\ind_{j}(t)}
	
	Where $\ind_i(t) := \ind_{t_{i}, t_{i+1}}(t)$. We now consider the following
	\begin{align*}
		\int_0^t(\alpha f(s) + \beta g(s))dW_s &= \int_0^t\left(\alpha \fsimple + \beta \gsimple \right) dW_s\\
		&= \sum_{k=0}^{n-1}\left(\alpha \fsimple + \beta \gsimple \right) \DeltaW[j]\\
		&= \alpha\sum_{k=0}^{n-1}\left(\fsimple\right)\Delta W_j + \\
		&\phantom{{}=1}\beta\sum_{k=0}^{n-1}\left(\gsimple\right)\DeltaW[j]\\
		&= \alpha\int_0^t f(s) dW_s + \beta \int_0^t g(s) \dW[s].
	\end{align*}
	
	2) For this next proof, we first note that $\Delta W_i$ is independent of its $\salg$. We can then write
	\begin{align}
		\E[\DeltaW| \salgF_{t_i}] &= \E[\DeltaW] = 0 \text{, and}\\
		\E[\DeltaW^2 | \salgF_{t_i}] &= \E[\DeltaW^2] = \E[\left(W_{t_{i+1}} - W_{t_{i}}\right)^2] = t_{i+1} - t_i.
	\end{align}
	Consider now
	\begin{align*}
	\Exp{f(t_i)\DeltaW} &= \Exp{\Exp{f(t_i)\DeltaW|\salgF_{t_i}}}\\
						&= \Exp{f(t_i)\Exp{\DeltaW|\salgF_{t_i}}}\\
						&= 0.
	\end{align*}
	It follows,
	\begin{align*}
		\Exp{I(f)} &= \Exp{\sum_{i=0}^{n-1}f(t_i)\DeltaW}\\
				   &= \sum_{i=0}^{n-1}\Exp{f(t_i)\DeltaW}\\
				   &= 0.
	\end{align*}
	
	We now consider
	\begin{align}
		\Exp{I(f)^2} &= \Exp{\left(\sum_{i=0}^{n-1} f(t_i) \DeltaW \right)^2}\label{eq:itois1} \\
					 &= \Exp{\sum_{i=0}^{n-1}f^2(t_i)\left(\DeltaW\right)^2 +  2\sum_{i<j}f(t_i)f(t_j) \DeltaW\DeltaW[j]}\nonumber \\
					 &= \sum_{i=0}^{n-1}\Exp{f^2(t_i)\left(\DeltaW\right)^2} + \label{eq:itois2}\\ 
					 &\phantom{{}=1} 2\sum_{i<j}\Exp{f(t_i)f(t_j) \DeltaW\DeltaW[j]}.\label{eq:itois3}
	\end{align}
	Where, (\ref{eq:itois3}):
	\begin{align*}
		\Exp{f(t_i)f(t_j) \DeltaW\DeltaW[j]} &= \Exp{\Exp{f(t_i)f(t_j) \DeltaW\DeltaW[j]|\salgF_{t_j}}}\\
			&= \Exp{f(t_i)f(t_j)\DeltaW\Exp{\DeltaW[j]|\salgF_{t_j}}}\\
			&= 0.
	\end{align*}
	
	We can rewrite (\ref{eq:itois1}) using only (\ref{eq:itois2}), i.e.,
	\begin{align*}
		\Exp{I(f)^2} &= \sum_{i=0}^{n-1}\Exp{f^2(t_i)\left(\DeltaW\right)^2}\\
		&= \sum_{i=0}^{n-1}\Exp{\Exp{f^2(t_i)\left(\DeltaW\right)^2|\salgF_{t_i}}} \\
		&= \sum_{i=0}^{n-1}\Exp{f^2(t_i)\Exp{\left(\DeltaW\right)^2|\salgF_{t_i}}} \\
		&= \sum_{i=0}^{n-1}\Exp{f^2(t_i)}\left(t_{i+1} - t_i\right) \\
		&= \int_0^t \Exp{f^2(s)} ds.
	\end{align*}
\end{proof}

\begin{remark}
	It\^o's integral as defined for $f\in\Mfam{a}{b}$ is a linear function, and indeed, an isometry since
	\begin{equation}
		\norm{I(f)}_{[a,b]} = \norm{f}_{[a, b]\times\Omega}.
	\end{equation}
	We can rewrite this last statement as
	\begin{equation}
		\Exp{\left(\int_0^t f dWs\right)} = \int_0^t\Exp{f^2(s)} ds = \Exp{\int_0^t f^2(s) ds}.
	\end{equation}	 
	
	Consequently, for any $f, g \in \Mfam{0}{t}$
	\begin{equation}
		\innerprod{I(f)}{I(g)}_{\Omega} = \innerprod{f}{g}_{\Omega\times[0, t]}.
	\end{equation}

To see why, we first note that $\innerprod{f}{g} = \int_0^t\Exp{f\cdot g}dt$. It follows that
\begin{align*}
	\innerprod{I(f)}{I(g)} &= \frac{1}{4}\left(\norm{I(f) + I(g)}^2 - \norm{I(f) - I(g)}^2\right)\\
						   &= \frac{1}{4}\left(\norm{I(f + g)}^2 - \norm{I(f - g)}^2\right)\\
						   &= \frac{1}{4}\left(\int_0^t \Exp{(f+g)^2}dt - \int_0^t \Exp{(f-g)^2}dt\right)\\
						   &= \frac{1}{4}\left(\int_0^t\Exp{f^2 + 2f\cdot g + g^2 - f^2 + 2f\cdot g - g^2} dt\right)\\
						   &= \int_0^t\Exp{f\cdot g} dt \\
						   &= \Exp{\int_0^t \left(f\cdot g\right) dt}\\
						   &= \innerprod{f}{g}.
\end{align*}
\end{remark}

%TODO: Add Proof
\begin{proposition}\label{prop:bounded_simple_borel}
	Let $f:[0, t]\to \RNums$ a Borel-measurable function such that $|f|\leq M$, and define $f_n:[0, t]\to \RNums$ as
	\begin{equation}
		f_n(\tau) := n\int_0^\tau e^{-n(\tau-s)}f(s) ds; \ 0 \leq \tau \leq t.
	\end{equation}
Then, $\{f_n\}$ is uniformly bounded by $M$ and $f_n \to f$ ($\mu$ a.e.) for every $\tau\in [0, t]$.
\end{proposition}

\begin{proposition}\label{prop:l2_convergence_ito_integral}
	The space $\Mfam{0}{t}$ is dense in $\Nfam{0}{t}$ under $L_2([a,b]\times\Omega)$. That is, for every $f\in\Nfam{0}{t}$, there exists a sequence $\{f_n\}_{n\geq 1} \in \Mfam{0}{t}$ such that
	\begin{equation}
		\Exp{\left(\int_0^t |f_n(s) - f(s)|dW_s\right)^2} \to 0.
	\end{equation}
	
\begin{proof}
We first note that 
\begin{equation}
	\Exp{\left(\int_0^t |f_n(s) - f(s)|dW_s\right)^2} = \Exp{\int_0^t\left|f_n(s) - f(s) \right|^2 ds} \to 0.
\end{equation}

\textbf{Step 1}: Continuous and bounded $f\in\Nfam{0}{t}$.\\
Let $g(\cdot, \omega)\in \Nfam{0}{t}$ continuous for each $\omega$. Then there exist a set $\{h_n\}_{n\geq 1}$ of simple processes in $\Mfam{0}{t}$ such that 
	\begin{equation}
		\Exp{\int_0^t\left(g - h_n\right)^2 ds}	\to 0.
	\end{equation}

Let $h_n(t, \omega) = \sum_{j=0}^{n-1}g(t_i)	\ind_{[t_i, t_{i+1}](t)}$. Then, $h_n$ is simple and,
\begin{equation}
  \lim_{n\to\infty} g - h_n = 0.
\end{equation}

Since $g - h_n$ converges to 0 (a.e.), the convergence is also in measure. Consequently, from the bounded convergence theorem we have that
\begin{equation}
  \lim_{n\to\infty} \int_0^t\left(g - h_n\right)^2 ds = 0 \ \forall \ \omega \in \Omega.
\end{equation}

Finally, by the dominated convergence theorem we see that
\begin{align*}
  \lim_{n\to\infty} \int_\Omega\left(\int_0^t\left(g - h_n\right)^2 ds\right)d\Pm &=  \int_\Omega \lim_{n\to\infty} \left(\int_0^t\left(g - h_n\right)^2 ds\right)d\Pm \\
  &= \Exp{\lim_{n\to\infty}\left(\int_0^t\left(g - h_n\right)^2 ds\right)} \\
  &= 0.
\end{align*}

\textbf{Step 2}: Bounded $f\in\Nfam{0}{t}$.\\
Let $f\in\Nfam{0}{t}$ be bounded i.e., there exists $M<\infty$ such that $|f(t, \omega) \leq M|$ for every $(t,\omega) \in [a,b]\times\Omega$. Then, there exists a sequence $\{f_n\}_{n\geq 1}$ of simple functions such that $f_n \to f$ in $L_2$.\\

By \ref{prop:bounded_simple_borel}, there exists $f_n$ such that $t\to f_n(t,\omega)$ is continuous for every $\omega\in\Omega$, $|f(t,\omega)|\leq M$, and $f_n(t,\Omega) \to f(t,\Omega)$. Since $f_n\to f$ a.e., then $f_n - f \to 0$ in measure, where we conclude by the bounded convergence theorem that
	\begin{equation}
		\Exp{\int_0^t(f_n - f)^2 ds} \to 0.
	\end{equation}

\textbf{Step 3}: Any $f\in\Nfam{0}{t}$.\\
For every $n\in\mathbb{N}$, define
	\begin{equation}
		f_n(t) = \begin{cases}
					f(t) & |f(t)|\leq n\\
					n 	 & f(t) > n \\
					-n   & f(t) < n
			     \end{cases}
	\end{equation}
It can be seen that $f_n\to f$. This implies a.e. convergence, and as a consequence, convergence in measure. Furthermore, $f_n$ is bounded by $f$ ($|f_n| \leq f$). The $L_2$ convergence follows from the dominated convergence.
\end{proof}
\end{proposition}

\begin{definition}[\textbf{It\^o's Integral for $f\in\Nfam{0}{t}$}]
	Let $f\in\Nfam{0}{t}$. By proposition \ref{prop:l2_convergence_ito_integral} there exists a sequence of simple processes $\{f_n\}_{n} \in \Mfam{0}{t}$ such that
	\begin{equation}
		\Exp{\int_0^t (f_n - f)^2 dt} \to 0.
	\end{equation}
	
	Then, the sequence $\{I(f_n)\}$ is a Cauchy sequence in $L_2\ProbSpace$ because
	\begin{align*}
		\Exp{(I(f_n) - I(f))^2} &= \Exp{\left|\int_0^t f_n dW - \int_0^t f dW\right|^2} \text{ (in $\ProbSpace$)} \\
								&= \Exp{\int_0^t(f_n - f)^2 ds} \text{ (in $([0,t]\times)\Omega,\mathscr B{([0,t])}\times\salgF, \lambda\times\Pm $)}.
	\end{align*}
	
	Therefore, there exists a random variable $I(f)\in L_2$ such that $I(f_n) \to I(f)$ in $L_2$. This limit is known as the \textbf{It\^o Integral}, and. we write:
	\begin{equation}
		I(f) := \int_0^t f(t, \omega) dW_t(\omega) = \lim_{n\to\infty}\int_0^t f_n(t,\omega) dW_t(\omega) \text{ (in $L_2$)}.
	\end{equation}
\end{definition}

\begin{proposition}[\textbf{Properties for $f$, $g$ $\in \Nfam{0}{t}$}]
	Let $f, g \in \Nfam{0}{t}$ then,
	\begin{equation}
		\Exp{\int_0^t f dW_s} = 0;
	\end{equation}
	
	\begin{equation}
		\Var{\int_0^t f dW_s} = \Exp{\left(\int_0^t f dW_s\right)} = \Exp{\int_0^t f^2 ds}\text{; and}
	\end{equation}
	
	\begin{equation}
		cov\left(\int_0^tfdW_s, \int_0^tgdW_s\right) = \int_0^t \Exp{f\cdot g} ds.
	\end{equation}
\end{proposition}

\begin{definition}[\textbf{Indefinte Integral}]
	Let $f\in \Nfam{0}{T}$, we define the indefinite It\^o integral
	\begin{equation}
		X(t) := \int_0^t f(s) dW_s \ \forall \ t\in[0, T].
	\end{equation}
\end{definition}

\begin{proposition}
	Let $f\in\Nfam{0}{T}$ and $X(t)$ an indefinite integral then,
	\begin{enumerate}
		\item $X(\cdot)$ is adapted to $\salgF$, i.e., $X(t)$ is $\salgF_t$-measurable for all $t\in[0, T]$; \label{it:adapted-int}
		\item $X(t)$ is an $L_2$ process and a $\salgF_t$-martingale
	\end{enumerate}
\end{proposition}

%TODO: Complete proof
\begin{proof}
For $f\in\Mfam{0}{t}$, \ref{it:adapted-int} follows from the definition of an It\^o Integral:
\begin{equation}
  \int_0^t f(s) dW_s = \sum_{i=0}^{n-1} f(t_i) \Delta W_i;
\end{equation}
for $f\in\Nfam{0}{t}$, \ref{it:adapted-int} follows considering its limit in $L_2$.
\end{proof}


%TODO: proof??
\begin{theorem}
	Let $f\in\Nfam{0}{T}$. There exists a $t$-continuous version of
	\begin{equation}
		\int_0^t f(s,\omega) dW_s(\omega) \ 0\leq t \leq T.
	\end{equation}
I.e., there exists a $t$-continuous stochastic process $Y_t$ defined over $\ProbSpace$ such that
\begin{equation}
	\Pm\left[Y_t = \int_0^tf(s,\omega) dW_s(\omega)\right] = 1 \ \forall \ 0\leq t \leq T.
\end{equation}
\end{theorem}

\section{Extensions of It\^o's Integral}
%TODO: the following section is dedicated to define a more general family of ito processes that those defined for N[0, t] (we want to introduce the need of a new family of proceses not in L2, and the multidimensional ito integral ).
\begin{definition}
	Let $W(\cdot) := \{W_t : t\geq 0\}$ a Brownian motion process over $\ProbSpace$ and let $\mathscr{G}$, a family of $\salg$s where
	\begin{enumerate}
		\item $\mathscr{G}(s) \subseteq \mathscr{G}(t)$ for all $0\leq s \leq t$;
		\item $W(\cdot)$ is adapted to $\mathscr{G}(\cdot)$, i.e.,
		\item For all $f\geq 0$ and $h > 0$, $W_{t+h} - W_t$ is independent of $\mathscr{G}(t)$.
	\end{enumerate}
\end{definition}

\section{It\^o's Formula}\label{th:itos_formula}
\begin{definition}[\textbf{It\^o Process}]
	let $W_t$ a standard Brownian motion on $\ProbSpace$. We define the It\^o process $X(t)$ over $\ProbSpace$ as
	\begin{equation}
		X(t) := \int_0^t\mu(s,\omega) ds  + \int_0^t\sigma(s,\omega) dW_s.
	\end{equation}
	
	Where
	\begin{enumerate}
		\item $\int_0^t \mu(s,\omega) ds < \infty$; and
		\item $\int_0^t\sigma(s,\omega) dW_s < \infty$.
	\end{enumerate}
	
	The It\^o Process can also be defined in a differential form as follows:
	\begin{equation}
		dX(t) = \mu(t, \omega) dt + \sigma(t, \omega) \dW[t].
	\end{equation}
\end{definition}

\begin{theorem}[\textbf{It\^o's Formula}]
	Let $X(t)$ be an It\^o process of the form
	\[
		dX(t) = \mu(t, \omega) dt + \sigma(t, \omega) dW_t,
	\]
	and $g(t, \omega) \in C^2([0, \infty]\times\RNums)$. Then,
	\begin{equation}
		Y(t) := g(t, X_t)
	\end{equation}
	is also an It\^o Process where
	
	\begin{equation}\label{eq:itos_formula}
		dY(t) = \partialwrt{g}{t}(t, X(t)) dt + \partialwrt{g}{x}(t, X(t)) \left(dX(t)\right) + \frac{1}{2}\partialwrt[2]{g}{x} (t, X(t)) \left(dX(t)\right)^2
	\end{equation}	
\end{theorem}

\begin{proposition}\label{prop:on_bounded_ito_g}
We may assume that $g$ and its two first derivatives are bounded in the following integral:
\begin{equation}\label{eq:g_bounded}
  g(0, X(0)) + \int_0^t \left(\partialwrt{g}{s} + \mu\partialwrt{g}{s} + \sigma^2\partialwrt[2]{g}{x}\right) ds + \int_0^t\sigma^2(s, \omega) dW_s
\end{equation}

Where $X(s) = \int_0^t \mu(s, \omega) ds + \int_0^t \sigma(s, \omega) dW_s$.

\begin{proof}
	For fixed $t\geq 0$ and $n\in\mathbb{N}$ we define $g_n(s, x) = g(s, x)$ for all $s \leq t$, $|x|\leq n$. For fixed $\omega$ define $\tau_n(\omega) = \tau_n := \inf\{s>0: |X(s,\omega)| \geq n\}$, and
	
	\begin{equation}\label{eq:equivalence_stop_time_integral}
		\int_0^{t} \sigma(s,\omega) \partialwrt{g_n}{x}(s, X(s)) \ind_{s\leq \tau_n} dW_s = \int_0^{t\wedge\tau_n} \sigma(s,\omega) \partialwrt{g_n}{x} (s, X(s)) dW_s.
	\end{equation}

Note that \ref{eq:equivalence_stop_time_integral} takes values up to $\min\{t, \tau_n\}$, which are the bounds defined for $g_n$ (it would be zero otherwise) then,

	\begin{equation}
	  \int_0^{t\wedge\tau_n} \sigma(s,\omega) \partialwrt{g_n}{x} (s, X(s)) dW_s = \int_0^{t\wedge\tau_n} \sigma(s,\omega) \partialwrt{g}{x} (s, X(s)) dW_s.
	\end{equation}
	

It follows from (\ref{eq:g_bounded}) that $g(t\wedge\tau_n, X(t\wedge\tau_n))$ equals
\begin{equation}
  g(0, X(0)) + \int_0^{t\wedge\tau_n} \left(\partialwrt{g}{s} + \mu\partialwrt{g}{s} + \sigma^2\partialwrt[2]{g}{x}\right) ds + \int_0^{t\wedge\tau_n}\sigma^2(s, \omega) dW_s
\end{equation}

Finally,

\begin{equation}
  \Pm(\tau_n \geq t) = 	\Pm(\inf	\{s: |X_s| \leq n\} > t) \to 1
\end{equation}

Which shows that $g$ is bounded for all $n \geq 1$.
\end{proof}
\end{proposition}

\begin{proof}[proof of It\^o's formula]
	In order to prove \ref{th:itos_formula} we will consider another representation of It\^o's formula replacing $dX(t) = \mu dt + \sigma dW_t$ in (\ref{eq:itos_formula}). It then follows that
	
	\begin{align*}
		&\partialwrt{g}{t}(t, X(t)) dt + \partialwrt{g}{x}(t, X(t)) \left(dX(t)\right) + \frac{1}{2}\partialwrt[2]{g}{x} (t, X(t)) \left(dX(t)\right)^2 \\
		&= \partialwrt{g}{t}(t, X(t)) dt + \partialwrt{g}{x}(t, X(t)) \left(\mu dt + \sigma dW_t\right) + \\
		&\phantom{{}=1} \frac{1}{2}\partialwrt[2]{g}{x} (t, X(t)) \left(\mu dt + \sigma dW_t\right)^2.
	\end{align*}
	
	Rearranging terms and integrating both sides, 
	\begin{equation}
  Y(t) = \int_0^t \left(\partialwrt{g}{s}{(s, X_s)} + \mu\partialwrt{g}{x}{(s, X_s)} + \frac{\sigma^2}{2}\partialwrt[2]{g}{x}{(s, X_s)}\right)ds + \int_0^t \sigma dW_t.
	\end{equation}

	Let us take into account $g_n \in C^2([0, \infty]\times\RNums)$ such that $g_n$ and its two first derivates are bounded for all $n$, and converge uniformly to $g$, $\partialwrt{g}{t}$, $\partialwrt{g}{x}$, and $\partialwrt[2]{g}{t}$. By \ref{prop:on_bounded_ito_g}, we can assume that such sequence of functions exists.\\
	
	Taking $g(t, X(t))$ and expanding as a Taylor series we see that:
	\begin{equation}\label{eq:taylor_g}
		g(t, X(t)) = g(0, X(0)) + \sum_{i=0}^{n-1}\Delta g(t_i, X_i);
	\end{equation}
	
	where 
	\[
		\Delta g(t_i, X_i) = \partialwrt{g}{t}\Delta t_i + \partialwrt{g}{t} \Delta X_i + \frac{1}{2}\partialwrt[2]{g}{t} (\Delta t_i)^2 + \frac{\partial^2 g}{\partial x \partial y}(\Delta t_i)(\Delta X_i) + R_i
	\]
	
	We note that, as $\Delta t_i \to 0$,
\begin{align} 
\partialwrt{g}{t} \Delta t_i &\to \partialwrt{g}{t} dt
\end{align}

\begin{align} 
\partialwrt{g}{x} \Delta X_i &= \partialwrt{g}{x}\left(\mu\Delta t_i + \sigma \Delta W_i \right) \nonumber \\
&\to  \partialwrt{g}{x}(\mu dt + \sigma dW_t)
\end{align}

\begin{align}
\frac{1}{2}\partialwrt[2]{g}{x} (\Delta X_i)^2 &= \frac{1}{2}\partialwrt[2]{g}{x} (\mu\Delta t_i + \sigma \Delta W_i)^2 \nonumber\\
&= \frac{1}{2}\partialwrt[2]{g}{x} (\mu^2 (\Delta t_i)^2 + 2\sigma\mu \Delta W_i \Delta t_i + \sigma^2 (\Delta W_i)^2) \nonumber\\
&\to \frac{1}{2} \sigma^2 dt
\end{align}

Finally, 
\begin{equation}
  R_i = \bigO((\Delta t_i)^3 + (\Delta t_i)^2(\Delta X_i) + (\Delta t_i)(\Delta X_i)^2 + (\Delta X_i)^3).
\end{equation}

Expanding $\Delta X_i$ and cancelling out terms we find out that

\begin{align}
  R_i &= \bigO((\Delta t_i)^3 + (\Delta t_i)^2(\Delta W_i)) + (\Delta t_i)(\Delta W_i))^2 + (\Delta W_i)^3) \nonumber \\
  	&= \bigO((\Delta t_i)^3 + (\Delta t_i)^2(\Delta W_i)) + (\Delta t_i)(\Delta t_i) + (\Delta W_i)^2(\Delta W_i))\nonumber \\
  	&= \bigO((\Delta t_i)^2(\Delta t_i + \Delta W_i + 1) + \Delta t_i \Delta W_i) \label{eq:residual_ito_formula}.
\end{align}

We note that $(\Delta t_i)^2 \to 0$ and $(\Delta t_i)(\Delta W_i) \to 0$, thus (\ref{eq:residual_ito_formula}) goes to 0.\\

We conclude that the limit (in $L_2$) of (\ref{eq:taylor_g}) is

\begin{align}
	g(t, X(t)) &= g(0, X_0) + \int_0^t\left( \partialwrt{g}{s}ds + \partialwrt{g}{x}{\mu ds} + \partialwrt{g}{x}{\sigma dW_s} + \frac{1}{2}\partialwrt[2]{g}{s}{\sigma^2 ds} \right)\\
			 &= g(0, X_0) + \int_0^t\left(\partialwrt{g}{s} + \mu \partialwrt{g}{x} +\sigma^2 \partialwrt[2]{g}{s} \right)ds + \int_0^t\sigma \partialwrt{g}{x}{dW_s}.
\end{align}
\end{proof}

\begin{theorem}[\textbf{The General It\^o Formula}]
	
\end{theorem}
\section{Stochastic Differential Equations}
\end{document}